% mnras_template.tex
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[a4paper,fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
%\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
%\usepackage{wasysym}
\usepackage{savesym}
\savesymbol{iint}
\savesymbol{iiint}
\savesymbol{iiiint}
\savesymbol{idotsint}
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared
\newcommand{\bz}{\bmath{z}}
\newcommand{\br}{\bmath{r}}
\newcommand{\bg}{\bmath{g}}
\newcommand{\bd}{\bmath{d}}
\newcommand{\bv}{\bmath{v}}
\newcommand{\conj}[1]{\overline{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Sparse Redundant Calibration]{SpARC: Sparse Redundant Calibration}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[K. T. Smith et al.]{
Keith T. Smith,$^{1}$\thanks{E-mail: mn@ras.org.uk (KTS)}
A. N. Other,$^{2}$
Third Author$^{2,3}$
and Fourth Author$^{3}$
\\
% List of institutions
$^{1}$Royal Astronomical Society, Burlington House, Piccadilly, London W1J 0BQ, UK\\
$^{2}$Department, Institution, Street Address, City Postal Code, Country\\
$^{3}$Another Department, Different Institution, Street Address, City Postal Code, Country
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2015}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
This is a simple template for authors to write new MNRAS papers.
The abstract should briefly describe the aims, methods, and main results of the paper.
It should be a single paragraph not more than 250 words (200 words for Letters).
No references should appear in the abstract.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
keyword1 -- keyword2 -- keyword3
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}

%All papers should start with an Introduction section, which sets the work
%in context, cites relevant earlier studies in the field by \citet{Others2013},
%and describes the problem the authors aim to solve \citep[e.g.][]{Author2012}.

\section{Historical Overview}

Write a little about how redundant calibration developed, a basic lit review.
Maybe write a little bit about LOGCAL here as well.

\subsection{EW regular-layout}
Describe the EW-layout and its redundant baselines

\subsection{LINCAL}
\begin{equation}
c_{ij} = y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}
\end{equation}

\begin{eqnarray}
\frac{c_{ij}}{\partial \eta_i} &=& y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\ 
\frac{c_{ij}}{\partial \varphi_i} &=&  -i y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\
\frac{c_{ij}}{\partial \eta_j} &=& y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\ 
\frac{c_{ij}}{\partial \varphi_j} &=&  i y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\
\frac{y_{i-j}}{\partial \varphi_j} &=&  e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}
\end{eqnarray}

The Wirtinger derivative is used in the last equation.

\begin{eqnarray}
c_{ij} &\approx& c_{ij}^0 + \Delta \eta_i(y_{i-j}^0 e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}) + \Delta \eta_j(y_{i-j}^0 e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0})\\
&& -i\Delta\varphi_i (y_{i-j}^0e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}) +i\Delta\varphi_j (y_{i-j}^0e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0})\\
&& y_{i-j}^1(y_{i-j}^0e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0})\\
&\approx&  c_{ij}^0 + e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}(y_{i-j}^1 + y_{i-j}^0( \Delta \eta_i+ \Delta \eta_j - i\Delta\varphi_i + i\Delta\varphi_i))
\end{eqnarray}

\begin{equation}
\label{eq:residual}
r_{ij} = \delta_{ij} = c_{ij}-c_{ij}^0 = e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}(y_{i-j}^1 + y_{i-j}^0( \Delta \eta_i+ \Delta \eta_j - i\Delta\varphi_i + i\Delta\varphi_i)) 
\end{equation}

Eq.~\ref{eq:residual} allows us to construct the following linear system:

\begin{equation}
\boldsymbol{J}[\boldsymbol{\Delta \eta},\boldsymbol{\Delta \varphi},\boldsymbol{\Delta y}]^T = \boldsymbol{r}, 
\end{equation}

where $\boldsymbol{J}$ is equal to 
\begin{equation}
\boldsymbol{J} = \bigg [\overbrace{\frac{c_{pq}}{\partial \eta_i}}^{i= 1\cdots N},~\overbrace{\frac{c_{pq}}{\partial \varphi_i}}^{i= 1\cdots N},~\overbrace{\frac{c_{pq}}{\partial y_{t}}}^{t=1\cdots r_s} \bigg ] \bigg \} [pq] = 1\cdots N_{\textrm{b}} (p<q) 
\end{equation}
or
\begin{equation}
\boldsymbol{J} = \bigg [\frac{c_{pq}}{\partial \boldsymbol{\eta}},~\frac{c_{pq}}{\partial \boldsymbol{\varphi}},~\frac{c_{pq}}{\partial \boldsymbol{y}} \bigg ] \bigg \} [pq] = 1\cdots N_{\textrm{b}} (p<q). 
\end{equation}

The last column is again a Wirtinger derivative.

Which means we can obtain the least-squares estimate as follows:

\begin{equation}
[\boldsymbol{\Delta \eta},\boldsymbol{\Delta \varphi},\boldsymbol{\Delta y}]^T = [\boldsymbol{J}^H\boldsymbol{J}]^{-1}\boldsymbol{J}^H\boldsymbol{r}.
\end{equation}

What is interesting about LINCAL is that it seems to be a combination of standard least-squares (splitting the problem into amplitude and phase) and 
complex least-squares. The gains are solved with a standard approach (splitting into amp and phase) and the redundant visibilities use a Wirtinger derivative. What is strange though is that in normal complex optimization, if we optimize towards a complex variable x we also need to optimize towards
its complex conjugate. My best guess is that LINCAL works in any case since the conjugate redundant baselines are not used to calibrate ($p<q$). The only difference between the framework I was using is that
we regard the whole problem as complex, we construct our Jacobian by calculating the derivative towards $g$ and its conjugate as well as $y$ and its conjugate - Smirnov \& Tasse (2016). 

Now that the relation between LINCAL and GN has been shown we can start focusing on the speedup that the conjugate gradient method provides as Liu (2010) originally proposed.
Also worth checking: whether using an LM step instead of a GN step improves your convergence properties.
Briefly review LINCAL here and show that it is equivalent to the GN algorithm.

\section{Complex Optimization}
Scalar direction independent calibration is achieved through solving the following minimization problem:
\begin{equation}
\min_{\bg}\|\br\| = \min_{\bg}\|\bd - \bv(\bg)\|, 
\end{equation}
where
\begin{enumerate}
 \item $\bd$ is the observed visibility vector, i.e. 
 \begin{equation}
  \bd = [d_{pq}] \big \} ~ [pq] = 1\cdots N ~ (p<q).
 \end{equation}
 The vector entry $d_{pq}$ denotes the observed visibility associated with the baseline formed by antenna $p$ and $q$, while $N$ denotes the number or antennas
 in the array. 
 \item $\bg = [g_1,g_2,\cdots,g_N]$ represents the instrumental complex antenna gain vector.
 \item $\bv(\bg)$ denotes the predicted visibility vector, i.e.
 \begin{equation}
  \bv(\bg) = [g_p y_{pq}\conj{g}_q]  \big \} ~ [pq] = 1\cdots N ~ (p<q).
 \end{equation}
 In Eq... $y_{pq}$ denotes the visibility which was created from a skymodel and is associated with antenna $p$ and $q$. [Need to rw this sentence] 
\end{enumerate}

The most standard way of tackling the calibration problem is to treat the real and imaginary parts of the residual (difference between observed and predicted visibilities) separately and then to simultaneously solve for the real and imaginary parts
of the gains. 
\begin{equation}
\min_{\hat{\bg}}\|\hat{\br}\| = \min_{\hat{\bg}}\|\hat{\bd} - \hat{\bv}(\hat{\bg})\|, 
\end{equation}
where $\hat{\bg} = \big[\mathcal{R}\{\bg\},\mathcal{I}\{\bg\}\big]^T$, still need to define the other vectors properly....

We have to revert to the above formulation as a direct Taylor expansion of v is not possible as v is nonanalytic in its argument.



\citet{Smirnov2015} have recently shown that calibration can be performed using the following complex framework 
\begin{equation}
\min_{\breve{\bg}}\|\breve{\br}\| = \min_{\breve{\bg}}\|\breve{\bd} - \breve{\bv}(\breve{\bg})\|, 
\end{equation}
where $\breve{\bg} = \big[\bg,\conj{\bg}]^T$, still need to define the other vectors properly....

The above formulation is based on the fact that v breve is analytic in its argument and its complex conjugate as a whole.

%The main difference between the traditional approach and the 
%complex optimization framework proposed by \citet{Smirnov2015} is that in their framework they no longer treat the real and imaginary parts of the residual separately which enables them to directly solve for the complex antenna gains themselves (as well as their
%complex conjugates). 


Recall that most optimization techniques require a gradient operator to enable it to function properly. The main reason why we have traditionally treated 
the real and imaginary parts of the residual separately is due to the fact that $f(z) = \conj{z}$ is not a differentiable complex function where $z\in\mathbb{C}$ and $\conj{(*)}$ denotes conjugation, i.e. $\frac{df(z)}{dz}$
does not exist, if we apply the standard definition of differentiation:
\begin{equation}
x
\end{equation}



The main reason that we do not need to treat the real and imaginary parts of the residual separately any more is due to the 
fact 

%A standard way to optimize a complex function $f(\bz)$ of $n$ complex variables $\bz\in\mathbb{Z}^n$ is to recast $f$ as  optimize the real and imaginary parts of  separately.

 

The main difference between complex optimization and the more traditional
approach to calibration is that in complex calibration we 


Describe complex optimization, Jacobian, Hessian etc... Benefits drawbacks compared to LINCAL... etc..

\subsection{Jacobian derivation}
\subsection{Hessian derivation}
\begin{equation}
\boldsymbol{H} = 
\begin{bmatrix}
\boldsymbol{A} & \boldsymbol{B}\\
\boldsymbol{B}^* & \boldsymbol{A}
\end{bmatrix}
\end{equation}

With 

\begin{equation}
\boldsymbol{A} = 
\begin{bmatrix}
\boldsymbol{C} & \boldsymbol{D}\\
\boldsymbol{D^H} & \boldsymbol{E}
\end{bmatrix}
\end{equation}

\begin{equation}
\boldsymbol{B} = 
\begin{bmatrix}
\boldsymbol{F} & \boldsymbol{G}\\
\boldsymbol{G}^T & \boldsymbol{0}
\end{bmatrix}
\end{equation}

\begin{equation}
[\boldsymbol{C}]_{ij} = 
\begin{cases}
 \sum_{k \neq i} \left | g_k \right |^2 \left | y_{|i-k|} \right |^2 & \textrm{if} ~ i=j\\
 0 & \textrm{otherwise}
\end{cases}
\end{equation}

\begin{equation}
[\boldsymbol{D}]_{ij} = 
\begin{cases}
 g_i y_j^*  \left | g_{i+j} \right |^2  & \textrm{if} ~ i<j\\
 0 & \textrm{otherwise}
\end{cases}
\end{equation}

\begin{equation}
[\boldsymbol{E}]_{ij} = 
\begin{cases}
 \sum_{pq \in \mathcal{I}_i} \left | g_p \right |^2 \left | g_q \right |^2  & \textrm{if} ~ i=j\\
 0 & \textrm{otherwise}
\end{cases}
\end{equation}

\begin{equation}
[\boldsymbol{F}]_{ij} = 
\begin{cases}
 g_i g_j  \left | y_{|i-j|} \right |^2  & \textrm{if} ~ i \neq j\\
 0 & \textrm{otherwise}
\end{cases}
\end{equation}

\begin{equation}
[\boldsymbol{G}]_{ij} = 
\begin{cases}
 g_i y_j  \left | g_{i-j} \right |^2  & \textrm{if} ~ i > j\\
 0 & \textrm{otherwise}
\end{cases}
\end{equation}

Moreover, 

\begin{equation}
\mathcal{I}_i = \left\{pq|(q - p) = i \right\}
\end{equation}

The dimensions of the matrices:

\begin{enumerate}
\item $\boldsymbol{H}$: is a $(4N-2)\times(4N-2)$ or a $P \times P$ matrix. $N$ denotes the number of antennas and $P$ denotes the number of parameters. This matrix
contains $6N^2 - 2N - 2$ non-zero entries. It contains $16N^2 - 16N + 4$ entries. We can construct this matrix with $4N^2 -  4N$ or $\frac{1}{4} P^2 -1$ elementary operations.
\item $\boldsymbol{A}$: is a $(2N-1)\times(2N-1)$ matrix. This matrix has $N^2 + N - 1$ non-zero entries. We can construct this matrix with $2\frac{1}{2} (N^2 -  N)$ elementary operations.  
\item $\boldsymbol{B}$: is a $(2N-1)\times(2N-1)$ matrix. This matrix has $2N^2 - 2N$ non-zero entries. We can construct this matrix with $1\frac{1}{2} (N^2 -  N)$ elementary operations.
\item $\boldsymbol{C}$: is a $N\times N$ matrix. This matrix has $N$ non-zero entries. We can construct this matrix with $N^2-N$ elementary operations.
\item $\boldsymbol{D}$: is a $N \times (N-1)$ matrix. This matrix has $\frac{1}{2} (N^2 -  N)$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations. 
\item $\boldsymbol{E}$: is a $(N-1) \times (N-1)$ matrix. This matrix has $(N-1)$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations.  
\item $\boldsymbol{F}$: is a $N \times N$ matrix. This matrix has $N^2 - N$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations.
\item $\boldsymbol{G}$: is a $N \times (N-1)$ matrix. This matrix has $\frac{1}{2} (N^2 -  N)$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations.
\item $\boldsymbol{0}$: is a $(N-1) \times (N-1)$ all zero matrix. 
\end{enumerate}

We can construct $\boldsymbol{C}-\boldsymbol{G}$ in $O(N^2)$ 


The sparsity ratio of $\boldsymbol{H}$ is equal to
\begin{equation}
\gamma_N = \frac{10N^2-14N+6}{16N^2-16N+4} 
\end{equation}

The asymptotic sparsity ratio of $\boldsymbol{H}$ is equal to

\begin{equation}
\gamma = \lim_{N\rightarrow \infty}\frac{5N^2-7N+3}{8N^2-8N+2} = \frac{5}{8}
\end{equation}

Asymptotically the computational cost of the matrix-vector product between $\boldsymbol{H}$ and a vector of appropriate size is $O(P^2)$. However, the computational cost converges 
very slowly to its asymptotic value, and in general is equal to $O(P^{k_N})$, where $k_N < 2$.

\begin{eqnarray}
P^{k_N}_N &=& (1 - \gamma_N)P^2_N\\
k_N &=& \log_{P_N}(1 - \gamma_N) + 2\\
k &=& \lim_{N\rightarrow \infty} \log_{P_N}(1 - \gamma_N) + 2 = 2
\end{eqnarray}

Moreover,

\begin{eqnarray}
P^{c_N}_N &=& \frac{1}{4} P_N^2 -1\\
c_N &=& \log_{P_N}\left (\frac{1}{4} P_N^2-1 \right )\\
c &=& \lim_{N\rightarrow \infty} \log_{P_N}\left (\frac{1}{4} P_N^2-1\right ) = 2
\end{eqnarray}

NB $P_c$ IS WRONG HERE DID NOT TAKE INTO ACCOUNT THE CONJUGANTION OF $\boldsymbol{B}$ (need multiply its computational complexity by two).

\subsection{Jacobian-residual product}

\subsection{Computational complexity using CG}
\subsubsection{Sparsity}
\subsubsection{Condition number}

\begin{enumerate}
 \item Plot eigenvalue distribution for different SNR and antenna numbers and different lambda values
 \item Plot computational complexity bound plus error reduction for different setups
 \item Plot number of iterations required by CG as a function of number of antennas
 \item Plot redundant baseline visibilities to show that they are approximately equal.
\end{enumerate}

NB:: Repeat the second last plot for the positive-semi definite case (GN), showing that even if the Hessian is singular we get fast convergence.

\section{Results: Regualar square grid}
If there is time

\section{Results: Hexanol grid}
NB:: Need to test if it works for this redundant layout.


\section{Conjugate Gradient method}
brief description and lit review of the method.
computational complexity of the algorithm.
sparsity and condition number.
% \section{Methods, Observations, Simulations etc.}
% 
% Normally the next section describes the techniques the authors used.
% It is frequently split into subsections, such as Section~\ref{sec:maths} below.
% 
% \subsection{Maths}
% \label{sec:maths} % used for referring to this section from elsewhere
% 
% Simple mathematics can be inserted into the flow of the text e.g. $2\times3=6$
% or $v=220$\,km\,s$^{-1}$, but more complicated expressions should be entered
% as a numbered equation:
% 
% \begin{equation}
%     x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}.
% 	\label{eq:quadratic}
% \end{equation}
% 
% Refer back to them as e.g. equation~(\ref{eq:quadratic}).
% 
% \subsection{Figures and tables}
% 
% Figures and tables should be placed at logical positions in the text. Don't
% worry about the exact layout, which will be handled by the publishers.
% 
% Figures are referred to as e.g. Fig.~\ref{fig:example_figure}, and tables as
% e.g. Table~\ref{tab:example_table}.
% 
% % Example figure
% \begin{figure}
% 	% To include a figure from a file named example.*
% 	% Allowable file formats are eps or ps if compiling using latex
% 	% or pdf, png, jpg if compiling using pdflatex
% 	\includegraphics[width=\columnwidth]{example}
%     \caption{This is an example figure. Captions appear below each figure.
% 	Give enough detail for the reader to understand what they're looking at,
% 	but leave detailed discussion to the main body of the text.}
%     \label{fig:example_figure}
% \end{figure}
% 
% % Example table
% \begin{table}
% 	\centering
% 	\caption{This is an example table. Captions appear above each table.
% 	Remember to define the quantities, symbols and units used.}
% 	\label{tab:example_table}
% 	\begin{tabular}{lccr} % four columns, alignment for each
% 		\hline
% 		A & B & C & D\\
% 		\hline
% 		1 & 2 & 3 & 4\\
% 		2 & 4 & 6 & 8\\
% 		3 & 5 & 7 & 9\\
% 		\hline
% 	\end{tabular}
% \end{table}
% 
% 
% \section{Conclusions}
% 
% The last numbered section should briefly summarise what has been done, and describe
% the final conclusions which the authors draw from their work.

\section*{Acknowledgements}

The Acknowledgements section is not numbered. Here you can thank helpful
colleagues, acknowledge funding agencies, telescopes and facilities used etc.
Try to keep it short.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:

%\bibliographystyle{mnras}
%\bibliography{example} % if your bibtex file is called example.bib

\bibliographystyle{mnras}
\bibliography{paper}
% Alternatively you could enter them by hand, like this:
% This method is tedious and prone to error if you have lots of references
%\begin{thebibliography}{99}
%\bibitem[\protect\citeauthoryear{Author}{2012}]{Author2012}
%Author A.~N., 2013, Journal of Improbable Astronomy, 1, 1
%\bibitem[\protect\citeauthoryear{Others}{2013}]{Others2013}
%Others S., 2012, Journal of Interesting Stuff, 17, 198
%\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Some extra material}

If you want to present additional material which would interrupt the flow of the main paper,
it can be placed in an Appendix which appears after the list of references.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
