% mn2esample.tex
%
% v2.1 released 22nd May 2002 (G. Hutton)
%
% The mnsample.tex file has been amended to highlight
% the proper use of LaTeX2e code with the class file
% and using natbib cross-referencing. These changes
% do not reflect the original paper by A. V. Raveendran.
%
% Previous versions of this sample document were
% compatible with the LaTeX 2.09 style file mn.sty
% v1.2 released 5th September 1994 (M. Reed)
% v1.1 released 18th July 1994
% v1.0 released 28th January 1994

\documentclass[useAMS,usenatbib]{mn2e}

% If your system does not have the AMS fonts version 2.0 installed, then
% remove the useAMS option.
%
% useAMS allows you to obtain upright Greek characters.
% e.g. \umu, \upi etc.  See the section on "Upright Greek characters" in
% this guide for further information.
%
% If you are using AMS 2.0 fonts, bold math letters/symbols are available
% at a larger range of sizes for NFSS release 1 and 2 (using \boldmath or
% preferably \bmath).
%
% The usenatbib command allows the use of Patrick Daly's natbib.sty for
% cross-referencing.
%
% If you wish to typeset the paper in Times font (if you do not have the
% PostScript Type 1 Computer Modern fonts you will need to do this to get
% smoother fonts in a PDF file) then uncomment the next line
% \usepackage{Times}

%%%%% AUTHORS - PLACE YOUR OWN MACROS HERE %%%%%
\usepackage{subfigure}
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{mathrsfs}

\newcommand{\bz}{\bmath{z}}
\newcommand{\bs}{\bmath{s}}
\newcommand{\bA}{\bmath{A}}
\newcommand{\bB}{\bmath{B}}
\newcommand{\bC}{\bmath{C}}
\newcommand{\bE}{\bmath{E}}
\newcommand{\bF}{\bmath{F}}
\newcommand{\bG}{\bmath{G}}
\newcommand{\br}{\bmath{r}}
\newcommand{\bg}{\bmath{g}}
\newcommand{\bd}{\bmath{d}}
\newcommand{\bv}{\bmath{v}}
\newcommand{\bn}{\bmath{n}}
\newcommand{\by}{\bmath{y}}
\newcommand{\bJ}{\bmath{J}}
\newcommand{\bD}{\bmath{D}}
\newcommand{\bH}{\bmath{H}}
\newcommand{\bN}{\bmath{N}}
\newcommand{\bM}{\bmath{M}}
\newcommand{\bO}{\bmath{O}}
\newcommand{\bP}{\bmath{P}}
\newcommand{\bQ}{\bmath{Q}}
\newcommand{\bR}{\bmath{R}}
\newcommand{\bI}{\bmath{I}}
\newcommand{\ba}{\bmath{a}}
\newcommand{\bb}{\bmath{b}}
\newcommand{\bx}{\bmath{x}}
\newcommand{\bp}{\bmath{p}}
\newcommand{\bmJ}{\bmath{\mathcal{J}}}
\newcommand{\bmH}{\bmath{\mathcal{H}}}
\newcommand{\bzero}{\bmath{0}}
%\newcommand{\bvarrho}{\bmath{\varrho}}
%\newcommand{\bnu}{\bmath{\nu}}
%\newcommand{\bvarphi}{\bmath{\varphi}}
\newcommand{\conj}[1]{\overline{#1}}

\newcommand{\aaps}{A\&AS}
\newcommand{\aap}{A\&A}
\newcommand{\mnras}{MNRAS}
\newcommand{\nat}{Nature}
\newcommand{\physrep}{Phys. Rep.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Unknown]{Redundant calibration as a complex optimization problem}
\author[A. V. Raveendran and A. N. Other]{A. V. Raveendran$^{1}$\thanks{E-mail:
email@address (AVR); otheremail@otheraddress (ANO)} and A. N.
Other$^{2}$\footnotemark[1]\thanks{This file has been amended to
highlight the proper use of \LaTeXe\ code with the class file.
These changes are for illustrative purposes and do not reflect the
original paper by A. V. Raveendran.}\\
$^{1}$Indian Institute of Astrophysics, Bangalore 560034, India\\
$^{2}$Building, Institute, Street Address, City, Code, Country}
\begin{document}

\date{Accepted 1988 December 15. Received 1988 December 14; in original form 1988 October 11}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2002}

\maketitle

\label{firstpage}

\begin{abstract}
Traditionally redundant calibration is implemented via a hierarchical algorithmic strategy, i.e. a basic \textsc{firstcal} is performed, followed by \textsc{logcal} and \textsc{lincal}.  
\textsc{logcal} requires the use of the logartihm, while \textsc{lincal} makes use of the gradient-based Gauss-Newton algorithm.
We show that instead of splitting our calibration problem into its phase and amplitude components,
which is the approach taken by \textsc{lincal}, we can treat the complex unknown model parameters and their complex conjugates as separate unknowns instead. 
We further propose the use of the Levenberg-Marquardt algorithm, instead of the Gauss-Newton algorithm, to improve the convergence properties of the proposed complex
framework, which in turn reduces the number of layers that redundant calibration pipelines would require. We also compare two previously proposed methods to speedup redundant calibration, namely the alternating direction implicit method and the preconditioned conjugate gradient method. We found that the alternating 
direction implicit method outperforms the preconditioned conjugate gradient method, however the preconditioned conjugate gradient method is more robust as it is more straightforward to adapt to other calibration use cases. 
We mainly focus on the complex framework in this paper rather than the amplitude and phase approach as it makes it trivial to relate the alternating direction implicit method 
and the Levenberg-Marquardt algorithm with each other.
\end{abstract}

%that \textsc{lincal} is effectively the non-linear least-squares gradient-based Gauss-Newton algorithm. Therefore, an easy way to reduce the number of layers used in performing redundant calibration is to employ the Levenberg-Marquardt algorithm instead (which would improve the convergence properties of stock standard \textsc{lincal})

\begin{keywords}
circumstellar matter -- infrared: stars.
\end{keywords}

\section{Introduction}
The propogation effects (i.e. the ionosphere, antenna gains, etc.) a celestial signal experiences along its propogation path is modelled with the radio interforemetric measurement equation (RIME) \citep{ME1,RRIME1}.
Calibration is the procedure by which we remove the errors that these propogation effects induce onto our intereferometric data and is normally accomplished using a 
least-squares solver. In the case where we only model the antenna gains, which is the most basic form of the RIME, we can define calibration as finding the antenna gains that minimizes the difference betweem our observed and predicted visibilities. 

There are two recent developments in skymodel-based calibration which should be explicitly mentioned at this point, since they form the foundation this paper builds on. The first is the discovery of \textsc{StEfCal} \citep{Mitchell:MWA-cal,Salvini2014}, which 
is an alternating direction implicit (ADI) skymodel-based calibration algorithm. It works by assuming the gains are known while solving for their conjugates and then performing the reverse. 
This procedure results in the linearization of the calibration algorithm and reduces the computational complexity of calibration from $\mathbb{O}(N^3)$ to $\mathbb{O}(N^2)$, where $N$ denotes the number of antennas in the array. 
assume the conugates are known and solve for the gains. The second development is the idea: to not break the calibration problem 
into its real and imaginary parts and then to solve for the real and imaginary parts of the unknown parameters seperately, but to rather treat the antenna gains and their 
conjugates as seperate parameters to estimate \citep{Smirnov2015}. \citet{Smirnov2015} also showed that the \textsc{StEfCal} algorithm itself is merely a special case of this
gain-and-conjugate approach. 

Two baselines are redundant if their beaseline difference vectors have the same lenght and orientation, which imply that they measure the same $uv$-modes. When we use a redundant array layout we dramatically 
reduce the number of unkowns. If our array is redundant enough we reduce the number of unknowns to such an extend that we are able to solve for the unknown 
observed visibilities themselves in addition to the antenna gains. Redundant calibration is the procedure by which we estimate the unknown observed visibilities in addition
to the antenna gains.

Redundant calibration was already quite popular in the 1980's. \citet{Noordam1982} used redundant calibration and the WSRT (Westerbork Synthesis Radio Telescope) to produce high-dynamic range images of 3C84.
The first implementations of redundant calibration made use of the logarithm function, because when the logarithm is applied to the scalar measurement equation, redundant 
calibration is transformed into a linear problem \citep{Wieringa1992,Camps2003,Liu2010}. This approach is commenly referred to as \textsc{logcal}.
We briefly discuss \textsc{logcal} in Appendix~\ref{sec:logcal}. The next improvement in redundant calibration was to perform a firt-order taylor expansion, which is equivalent to the Gauss-Newton (GN) algorithm \citep{Kurien2016}, of the measurement equation an then to solve for the
perturbations of the model-parameters \citep{Liu2010}. The implementation of  this approach is called \textsc{lincal}. \textsc{lincal} is derived from first principals in Appendix~\ref{sec:lincal}. 
\textsc{logcal} and \textsc{lincal} are both implemented in \textsc{omnical}\footnote{https://github.com/jeffzhen/omnical}, which is a layered redundant calibration pipeline \citep{Zheng2014,Ali2015}.
The solutions of \textsc{logcal} are used as initial input for \textsc{lincal}, since the GN algorithm requires that its starting parameter guesses be in the vacinity 
of the solution.

Two ways of speeding up redundant calibration have also been proposed recently. The first technique makes use of the ADI method, in which we alternate between solving the gains, their conjugates and 
the true observed visibilities \citep{Wijnholds2012,Marthi2014}. The second is to use the preconditioned conjugate gradient (PCG) method. PCG allows us to exploit the sparsity inherent in the redundant calibration problem, 
to speed it up \citep{Liu2010}. A proper comparison between these two approaches has not been done before.

New instruments like LOFAR (Low Frequency Array) \citep{Noorishad2012}, PAPER (Precision Array for Probing the Epoch of Reionization) \citep{Ali2015} and HERA (Hydrogen Epoch of Reionization Array) \citep{deboer2015} have recently sparked a renewed interest in redundant calibration. Using redundancy is especially advantageous when we are trying to detect the EoR (Epoch of Reionization) 21 cm signal,
which is inherently quite faint \citep{Parsons2012}. Since redundant baselines make statistically independent measurements of the same $uv$-modes redundancy can greatly 
improve the SNR (signal-to-noise ratio) of EoR detection experiments. This explains why PAPER and HERA, are highly redundant arrays, as they are both primarily used to detect the EoR 21 cm signal.
[GIANNI TO REWRITE AND EXTEND EOR SECTION]

In this paper we propose to use the complex framework introduced by \citet{Smirnov2015} instead of the traditional amplitude and phase approach used by \textsc{lincal}.
We also propose to use the Levenberg-Marquardt gradient-based algorithm instead of the Gauss-Newton algorithm as it exhibits better convergence properties.
In doing so the amount of layers that redundant calibration pipelines need are effectively reduced. We also compare two recent redundant calibration speedup algorithms
with each other, namely ADI \citep{Marthi2014} and PCG \citep{Liu2010}. In this paper, we decided to mainly focus on the complex framework proposed by \citet{Smirnov2015} rather than the traditional phase and amplitude approach as it 
allows us to trivially prove that the ADI method proposed by \citep{Marthi2014} is merely a special case of the more general LM algorithm. Moreover, this framework also 
allows us to show that the ADI method proposed by \citep{Marthi2014} can for all intents and purposes be renamed to redundant \textsc{StEfCal} \citep{Mitchell:MWA-cal,Salvini2014}. 

ADD SUMMARY PARAGRAPH WHICH DEPENDS ON PAPER LAYOUT

% Main contributions of the paper:
% 
% \begin{enumerate}
%  \item Present a general framework which unifies all the techniques developed so far showing they are all related (\textsc{lincal}, non-linear estimator, etc ...). They are all non-linear
%  least-squares techniques, i.e. they either employ Gauss-Newton or the Levenberg-Marquardt algorithms. We have to start with \textsc{lincal} showing that it is GN, then we have to motivate 
%  why we want to use Oleg's complex optimization framework.
%  \item Use Oleg's complex optimization framework to re-derive the non-linear technique proposed by Marthi and Chengular. The novelty lies, in the fact that by using Oleg's
%  framework we can find analytic expressions for the Jacobian, the Hessian and the Jacobian-residual product (which is not even the case for \textsc{lincal}). State that the algorithm is 
%  effectively related to SteFCal and is eff an independent rediscovery of SteFCal and an extension of it into the redundant domain.
%  \item Also we present the array geometry function to help us make the derivations from Marthi and Chengular easier to read and understand for a general layout.
%  \item We also mention at this point that in Oleg's paper the question is raised is there a fast way of computing the exact inverse, we then present this new method, which is called
%  conjugate gradient method. We discuss the algorithm and the two things which bound its execution time. Which is $kappa$ and $m$ (spectral condition number and its sparsity). Then
%  we explore both of these parameters.
%  \end{enumerate}
%  
%  **************
%  FLOW OF PAPER
%  **************
%  
%  \begin{enumerate}
%  \item We need the definition of visibilities as in Liu. DEFINE SNR HERE ALREADY together maybe with the sigma value of the noise.
%  \item Introduce the array geometery function.
%  \item Write down logcal and lincal in matrix form... ?
%  \item Short discussion of least squares and jacobian and hessian's. General GN and LM update rules.
%  \item Introduce redundant calibration as a least squares problem. We will use this general fact to derive both popular methods.
%  \item Propose a possible solution witch leads to lincal. Mention that this approach works as in this form the function is differentiable (a taylor expansion in the 
%  parameters exists). Maybe mention that we can also divide the problem into real and imaginary etc...
%  \item Show how this relates to lincal for example ---> show lincal is GN.
%  \item Now introduce complex optimization ---> Main motivation for switching to the alternative framework is that the the differentials are very simplistic. We wish to show that
%  we can derive the method of Chengular.
%  \item Do the derivation of Chengular. 
%  \item Brief discussion abouth Chengular and SteFCal and Complex Optimization. Here I show that in Oleg's paper he re-derives an algorithm called SteFCal. Stefcal works
%  on the basis of alternating direction implicit method. The first signs of achieving a similar algorithm already appeared in Stefan's conference paper in which the alternating
%  idea was first proposed. Then I mention Chegular re-derives the expression by using partial derivatives and extends it to redundant. One could also have used the linear alternating
%  approach. In an attempt to merge the terminology that has independently develop in the general calibration and redundant calibration literature and to emphasize the close
%  relationship between Stefcal and the approach derived by I will use the label Redunandat SteFCal (R-Stefcal) to refer to the NLS method proposed by ... Lastly we mention that similarly
%  to how Oleg re-derived stefcal in, we have achieved the same approach.
%  \item Faster Exact inverse. A question that Oleg poses in his paper is, does there exist a faster way to take the exact inverse of JhJ? One that is almost linear, and
%  implies that we can therefore implement the full LM algorithm. The aim here is of course to reduce the number of iterations that are required to converge by using the 
%  full inverse instead which would hopefully provide enough of a speedup to compensate for the more expensive full-inverse. The algorithm we propose is the conjugate 
%  gradient method.
%  \item We give the images of the HESSIAN of both lincal and the complex method (number of terms). So we can mention that both are sparse and contain diagonal entries that 
%  are more significant than the off-diagonal entries. What linear inversion approach can take advantage of both these phenomenon. One such technique is 
%  cg. 
%  \item Briefly discuss CG and how its computationally bounded by its condition number and its sparsity (how does the diagonal play a role).
%  \item Simulation description
%  \item Will CG improve things?
%  \item Now first discuss the condition
%  number of the Hessian before and after pre-conditioning (pre-codnitioning can only be applied if a good inverse of a matrix is known, if it is known then it can improve 
%  the spectral condition number of a matrix. We present here the kappa and iteration number graphs for the HEXAGONAL layout. Although the
%  \item Now we discuss the sparsity results. 
%  \item Provide a table that theoretical compares R-StEFCal and SPARC.
%  \item Number of outerloop iterations. 
%  \item Timing results.
%  \item Accuracy results.
%  \item Maybe some freq simulations.
%  \end{enumerate}
%  
%  We
 
\section{Preliminaries}
There are two major pre-existing algorithms we use throughout this paper, namely the Conjugate Gradient (CG) method and least-squares parameter estimation.
We respectively discuss the CG method and least-squares in Section~\ref{sec:conj_grad} and Section~\ref{sec:ls}.

\subsection{Conjugate Gradient Method}
\label{sec:conj_grad}
The CG method was independently discovered by \citet{Hestenes1973} and \citet{Stiefel1952}. They later published a joint paper, which is now considered as the seminal
reference on CG \citep{Hestenes1952}. The iterative CG method is used to solve a particular class of linear system. The CG method is generally used to solve
\begin{equation}
\label{eq:linear_system}
\bb = \bA\bx,
\end{equation}
where $\bb\in\mathbb{C}^P$ is a known complex column vector of size $P$, $\bx\in\mathbb{C}^P$ is an unknown complex column vector with the same length as $\bb$, and $\bA\in\mathbb{C}^{P\times P}$ is a square positive-definite Hermitian matrix with the appropriate dimensions.  
Moreover, using the CG method to solve Eq.~\eqref{eq:linear_system} will only be computationally advantageous if $\bA$ is also sparse (contain many zero entries).
Using the iterative CG method to solve large sparse linear sytems was popularized by \citet{Reid1971}. A good tutorial on the CG method can be found in \citep{Shewchuk1994}.
The iterative CG method is presented in Algorithm \ref{algo:CG}. 

%If $\bA$ is a square positive semi-definite Hermitian matrix, then the conjugate gradient method only converges if $\bb$ is in the column range of $\bA$ (cite Lu2016).

\begin{algorithm}
\caption{Conjugate Gradient Method. Inputs: $\bA$, $\bb$, a starting value for $\bx$, maximum number of iterations $k_{\textrm{max}}$ and an error tolerance $\epsilon<1$ Output: $\bx$ the solution to Eq.~\eqref{eq:linear_system}. \citep{Shewchuk1994}.}\label{algo:CG}
\begin{algorithmic}[1]
\State $k \gets 0$
\State $\br \gets \bb - \bA\bx$
\State $\bp \gets \br$
\State $\delta_{\textrm{new}} \gets \br^H\br$
\State $\delta_0 \gets \delta_{\textrm{new}}$
\While{$k\leq k_{\textrm{max}}$ and $\delta_{\textrm{new}} > \epsilon^2\delta_0$}
\State $\alpha_k \gets \frac{\delta_{\textrm{new}}}{\bp^H\bA\bp}$
\State $\bx \gets \bx + \alpha_k\bp$
\State $\br \gets \br - \alpha\bA\bp$
\State $\delta_{\textrm{old}} \gets \delta_{\textrm{new}}$ 
\State $\delta_{\textrm{new}} \gets \br^H\br$
\State $\beta_{k} \gets \frac{\delta_{\textrm{new}}}{\delta_{\textrm{old}}}$
\State $\bp \gets \br + \beta\bp$
\State $k \gets k + 1$
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsubsection{Computational Complexity}
\citet{Kaniel1966} was able to determine convergence bounds for CG using Chebyshev polynomials. A much more in depth analysis of CG convergence can be found in
\citep{Sluis1986}. As can be seen from Algorithm \ref{algo:CG}, the CG method makes use of a major outer loop (line 6 of Algorithm \ref{algo:CG}). It turns out that at worst 
the number of major iterations that CG requires to converge\footnote{If the $\bA$-norm is used to calculate the error.} is proportional to $\sqrt{\kappa}$, where $\kappa$ is the spectral condition number of the matrix $\bA$.
The mathematical definition of $\kappa(\bA)$ is
\begin{equation}
\label{eq:kappa}
\kappa(\bA) = \frac{\lambda_{\textrm{max}}}{\lambda_{\textrm{min}}}, 
\end{equation}
where $\lambda_{\textrm{max}}$ and $\lambda_{\textrm{min}}$ respectively denote the largest and the smallest eigenvalue of $\bA$.
Moreover, the most expensive operation that is conducted within each major loop of Algorithm~\ref{algo:CG} is the vector-matrix product $\bA\bp$, which has a complexity of $\mathbb{O}(m)$, where $m$ is the number of non-zero entries in $\bA$.
The computational complexity of CG is therefore 
\begin{equation}
\label{eq:cg_bound}
\mathbb{O}(\sqrt{\kappa}m). 
\end{equation}
It should now also be clear to the reader why the CG method is especially applicable to sparse linear systems. In sparse linear systems $m < P^2$.

\subsubsection{Preconditioning}
\label{sec:precon}
Preconditioning is a technique which allows us to improve the condition number of a matrix. Let $\bM$ be a positive-definite Hermitian matrix.
We can now indirectly solve for $\bx$ by using 
\begin{equation}
\label{eq:prec}
\bM^{-1}\bA\bx = \bM^{-1}\bb.
\end{equation}
If $\bM$ is a good preconditioner then $\kappa(\bM^{-1}\bA) \ll \kappa(\bA)$, i.e. a good preconditioner lowers the condition number of a matrix which in turn increases the convergence speed 
of CG (see Eq.~\eqref{eq:cg_bound}). The best possible choice of $\bM$ would therefore be $\bA$, which would lead to $\kappa$ becoming unity (as the largest and smallest eigenvalue of the Identity matrix is equal to one).
If $\kappa$ is one then Eq.~\eqref{eq:cg_bound} simplifies and becomes $\mathbb{O}(m)$. However, if we knew the inverse of $\bA$ then obtaining $\bx$ would be trivial and there would be no need for the CG method in the first place. 
In short, a good preconditioner $\bM$ must approximate $\bA$ fairly accurately. However, if it is not much less expensive to 
compute the inverse of $\bM$ than that of $\bA$ we actually gain nothing (see Eq.~\eqref{eq:prec}).  

If the diagonal entries of $\bA$ is much more significant than its off-diagonal entries then the 
Jacobian preconditioner is a good choice of $\bM$. The Jacobian preconditioner is computed as follows:
\begin{equation}
\bM = \bA\odot\bI, 
\end{equation}
where $\bI$ is the identity matrix and $\odot$ denotes the Hadamard product. Since the Jacobian preconditioner is a diagonal matrix its inverse can be obtained trivially.

\subsubsection{Normal Equation}
\label{sec:normal}
Lets assume that instead of Eq.~\eqref{eq:linear_system} we have a more general linear system, i.e.
\begin{equation}
 \bb = \bB\bx,
\end{equation}
where $\bb\in\mathbb{C}^K$, $\bx\in\mathbb{C}^P$  and $\bB\in\mathbb{C}^{K \times P}$, with $K > P$. We can use the normal equation 
\begin{equation}
\label{eq:normal_equation}
\bB^H\bB\bx = \bB^H\bb, 
\end{equation}
to estimate the $\bx$ that minimizes
\begin{equation}
\label{eq:norm_ls}
\|\bb-\bB\bx\|_F^2. 
\end{equation}
In this paper, we denote the Hermitian transpose with $(*)^H$ and the Frobenius norm with $\|*\|_F$.

There are a few important observations that we can make while inspecting Eq.~\eqref{eq:normal_equation}:
\begin{enumerate}
\item $\bB^H\bB$ is a square positive semi-definite Hermitian matrix by construction.
\item $\bB^H\bb$ is in the column range of $\bB^H\bB$; this follows trivially from 
\begin{equation}
\bB^H\bb \in \mathcal{R}(\bB^H) \implies \bB^H\bb \in \mathcal{R}(\bB^H\bB).   
\end{equation}
\end{enumerate}
In this paper, we denote the column range of a matrix $\bA$ with $\mathcal{R}(\bA)$.

Recall that we mentioned that in general we can only solve Eq.~\eqref{eq:linear_system} with CG if $\bA$ is a square positive-definite Hermitian matrix. The CG method actually has an even broader applicability, it can be used to solve Eq.~\eqref{eq:linear_system} even if $\bA$ is an Hermitian positive semi-definite
square matrix as long as $\bb$ is in the column range of $\bA$ \citep{Lu2015}. In light of \citet{Lu2015}, the two observations in the numbered list we presented in the previous paragraph imply that Eq.~\eqref{eq:normal_equation}
can be solved by employing the CG method. The solution returned by CG is however only unique if $\bB^H\bB$ is positive-definite (invertible). The solution
obtained by CG, whether it be unique or not, does however minimize Eq.~\eqref{eq:norm_ls} in a least squares sense.

Note that if $\bA$ is positive semi-definite its smallest eigenvalue is zero, which implies that Eq.~\eqref{eq:kappa} is not defined. \citet{Lu2015} show that when $\bA$
is a square positive semi-definite Hermitian matrix we do not use the spectral condition number to bound the convergence complexity of CG, but the general spectral condition
number instead. The general spectral condition number is very similar to the spectral condition number; the only difference being that in Eq.~\eqref{eq:kappa} we replace  
$\lambda_{\textrm{min}}$ with $\lambda_{\textrm{min}}^+$, which denotes the smallest positive eigenvalue. 

\subsection{Least Squares}
\label{sec:ls}
The method of nonlinear least squares is tanatamount to solving the following optimization problem:  
\begin{equation}
\label{eq:least_squares}
\min_{\bzeta} \Lambda(\bzeta) = \min_{\bzeta} \|\brho\|_F^2 = \min_{\bzeta} \|\bdelta - \bnu(\bzeta)\|_F^2, 
\end{equation}
where $\Lambda$ is known as the objective function, $\brho$ is the residual vector, $\bdelta$ is the observed data vector, $\bnu$ is the model-predicted data vector and $\bzeta$ is the model-parameter vector that minimizes Eq.~\eqref{eq:least_squares}.
In Eq.~\eqref{eq:least_squares}, we assume that the optimization problem is real valued.
The main objective of least squares is to find the model-parameters which minimizes the squared difference between the observed data and the model-predicted data.

The most standard way of solving Eq.~\eqref{eq:least_squares} is to use a gradient type of minimization algorithm. In this paper we will be focusing on the following gradient minimization algorithms: Gauss-Newton and Levenberg-Marquardt \citep{Levenberg1944,Marquardt1963}. 
The GN update step is defined as
\begin{equation}
 \Delta \bzeta = (\bJ^T\bJ)^{-1} \bJ^T\brho,
\end{equation}
while the LM update takes the following form
\begin{equation}
 \Delta \bzeta = (\bJ^T\bJ+\lambda\bD)^{-1} \bJ^T\brho,
\end{equation}
where $\bJ$ is the Jacobian matrix (i.e. the derivative of the model $\bnu$ with respect to the parameter vector $\bzeta$), $\lambda$ denotes the 
damping factor, while $\bD$ is a diagonal matrix whose diagonal entries are equal to the diagonal entries of $\bJ^T\bJ$. In this paper we respectively denote 
matrix transposition and matrix inversion with $(*)^T$ and $(*)^{-1}$.
These update steps can be used in an iterative manner to determine new estimates of $\bzeta$.

The gradient based minimization algorithms that are generally used to solve least squares problems (i.e. GN and LM) generally require the model $\bnu$ to be differentiable
towards each model parameter. When the least squares problem is complex it becomes less straightforward to apply these gradient based minimization methods. The reason being,
many complex functions are not analytic (no Taylor expansion around a fiducial point exists) and therefore not differentiable if the classic notion of differentiation is used. For instance if the 
classical definition of differentiation is used then $\frac{\partial \conj{z}}{\partial z}$, where $z \in \mathbb{C}$, does not exist.

The only way to circumvent the differentiability conundrum, associated with complex least square problems, is to recast our complex optimization problem in such a way that $\bnu$ becomes analytic in its argument.
The standard way of achieving this is to divide the complex optimization problem into its real and imaginary parts and then to solve for the real and imaginary parts of the parameters separately.

Recently, \citet{Sorber2012} proposed an alternative strategy, which involves the use of Wirtinger calculus \citep{Wirtinger1927}. The Wirtinger derivatives 
are defined to be 
\begin{align}
\label{eq:wir}
\frac{\partial}{\partial z} &= \frac{1}{2}\left ( \frac{\partial}{\partial x} -  i \frac{\partial}{\partial y} \right ),&\frac{\partial}{\partial \conj{z}} &= \frac{1}{2}\left ( \frac{\partial}{\partial x} +  i \frac{\partial}{\partial y} \right ). 
\end{align}
Using the above definitions we can now easily compute the following:
\begin{align}
\frac{\partial z}{\partial z} & = 1, & \frac{\partial \conj{z}}{\partial z}&=0, & \frac{\partial z}{\partial \conj{z}} & = 0, & \frac{\partial \conj{z}}{\partial \conj{z}}&=1.
\end{align}
In this alternative approach we treat the complex parameters and their conjugates as separate parameters.

\section{Problem Statement}
In Section~\ref{sec:s_mes}, we present the scalar measurement equation. Next, we define some indexing functions in Section~\ref{sec:indexing}. The measurement 
equation and the indexing functions from Section~\ref{sec:indexing} are then used in Section~\ref{sec:r_mes} to phrase redundant calibration as a least-squares problem, i.e. Eq.~\eqref{eq:least_squares_red}.
We present the Signal-to-noise ratio (SNR) definition we used in this paper in Section\ref{sec:snr}. The general scope and limitations of this paper is discussed in Section~\ref{sec:scope}.

\subsection{Scalar Measurement Equation}
\label{sec:s_mes}
The observed visibility $d_{pq}$ measured by the baseline formed by antenna $p$ and $q$ is described by
\begin{equation}
\label{eq:vis_definition}
d_{pq} = g_{p}\conj{g_q}y_{pq} + n_{pq},
\end{equation}
where $g_{p}$ and $g_{q}$ are the direction independent gains associated with antenna $p$ and $q$, $y_{pq}$ denotes the true visibility that baseline $pq$ measured
and $n_{pq}$ is the thermal noise component. Eq.~\eqref{eq:vis_definition} is known as the scalar measurement equation. Conjugation is denoted by $\conj{(*)}$. The true value of $g_p$, $g_q$ and $y_{pq}$ are unknown. We normally try to estimate the true value of
$g_p$, $g_q$ and $y_{pq}$ by calibrating our data and then imaging the result.

The real and imaginary components of the thermal noise is normally distributed with a mean of zero and a standard deviation proportional to   
\begin{equation}
\sigma \propto \frac{T_{\textrm{sys}}}{\sqrt{\Delta \nu \tau}}, 
\end{equation}
where $T_{\textrm{sys}}$ is equal to the system temperature, $\Delta \nu$ is the observational bandwidth and $\tau$ is the integration time per visibility. 

\subsection{Indexing Functions}
\label{sec:indexing}
In general there are two ways of indexing the true visibilities associated with different baselines. We can use the composite index $pq$, which is the strategy that is followed in Eq.~\eqref{eq:vis_definition},
or we can use a single unique baseline index which we assign in numerical order to each composite index; assuming the composite indexes are arranged in standard baseline order.
The composite indexes $pq$ are in standard baseline order when they are arranged as follow: $12, 13, 1N, 23\cdots$. Let 
\begin{equation}
\alpha_{pq} =
\begin{cases}
(q-p) + (p-1)\left (N-\frac{1}{2}p \right ) & \textrm{if}~p<q\\
%B + (p-q) + (q-1)\left (N-\frac{1}{2}q \right )) & \textrm{if}~p>q\\
%B+q + \frac{1}{2}(p-1)(p-2) & \textrm{if}~p>q\\
0 & \textrm{otherwise}
\end{cases}.
\end{equation}
We can use $\alpha_{pq}$ to map composite indexes to unique baseline indexes, i.e. 
\begin{equation}
12,13,\cdots,1N,23,\cdots \xrightarrow{\alpha_{pq}} 1,2,3,\cdots,B.
\end{equation}
We denote the number of baselines in the array with $B$, i.e. $B = \frac{N^2-N}{2}$.

Recall that redundant baselines sample the exact same visibilities in the $uv$-plane, i.e. if baseline $pq$ and $rs$ are redundant then $y_{pq} = y_{rs}$. 
The aforementioned fact allows us to group all the redundant baselines in a redundant array into the same redundant baseline group. When our array 
is a redundant array we therefore only require single index values for each redundant baseline group, instead of a unique single index label for 
each baseline. Let $\phi_{pq}$ be the function that maps composite antenna pairs to the appropriate single redundant baseline group index. Note that $\phi_{pq}$ is not symmetric as 
$\phi_{pq} = 0$ if $p>q$. We can also define the following symmetric variant of $\phi_{pq}$:
\begin{equation}
\zeta_{pq} = 
\begin{cases}
\phi_{pq}~\textrm{if}~p \leq q\\
\phi_{qp}~\textrm{if}~p>q
\end{cases}.
\end{equation} 
It is possible to construct a simple analytic expression for $\zeta_{pq}$ when our array is in an east-west regular configuration, namely $\zeta_{pq} = |q-p|$. 
It becomes increasingly more difficult to construct analytic expressions of $\zeta_{pq}$ for other more complicated array layouts. 
We will refer to $\zeta_{pq}$ as the symmetric geometric function. The empirically constructed symmetric geometry
functions of three different redundant layouts are depicted in Fig.~\ref{fig:geometry_function}. We denote the range of $\zeta_{pq}$ with $\mathcal{R}(\zeta_{pq})$. The maximal element 
that $\zeta_{pq}$ can ascertain is denoted by $L$ and can be construed as the maximal number of unique redundant baseline groups which can be formed for a given 
array layout. Alternatively, we can interpret the symmetric geometry functions in Fig.~\ref{fig:geometry_function} as geometry matrices. In this alternative paradigm, $\zeta_{pq}$ denotes a matrix entry instead. The dimension of these geometry 
matrices are $N\times N$. 

\begin{figure*}
\centering
\subfigure[Hexagonal layout]
{\includegraphics[width=0.33\textwidth]{./HEX_lay.pdf}\label{fig:HEX_lay}}
\subfigure[Square layout]
{\includegraphics[width=0.33\textwidth]{./SQR_lay.pdf}\label{fig:SQR_lay}}
\subfigure[Regular east-west layout]
{\includegraphics[width=0.33\textwidth]{./REG_lay.pdf}\label{fig:REG_lay}}

\subfigure[Hexagonal: $\zeta_{pq}$]
{\includegraphics[width=0.33\textwidth]{./HEX_phi.pdf}\label{fig:HEX_phi}}
\subfigure[Square: $\zeta_{pq}$]
{\includegraphics[width=0.33\textwidth]{./SQR_phi.pdf}\label{fig:SQR_phi}}
\subfigure[Regular east-west: $\zeta_{pq}$]
{\includegraphics[width=0.33\textwidth]{./REG_phi.pdf}\label{fig:REG_phi}}
\caption{In the top panel we have three different redundant antenna layouts, namely a hexagonal (left), square (middle) and a regular east-west (right) layout. 
In the bottom panel we have the respective symmetric redundancy geometry functions for the layouts on the top panel. We used 91 antennas
to construct the the hexagonal layout, while a 100 antennas where used in the square and east-west layouts. The maximal amount of redundant baselines that can be formed for 
the hexagonal, square and east-west layouts in the top panel are 165, 180 and 99 baselines respectively.\label{fig:geometry_function}}
\end{figure*}

If the function $\phi_{pq}$ is known and one is given one of the two antenna indexes that together form a redundant baseline as well as the redundant baseline group index itself then it is possible 
to calculate the unknown antenna index. This can be computed via the following two expressions:
\begin{equation}
\xi_{ij} = 
\begin{cases}
p~\textrm{if}~\exists! ~ p \in \mathbb{N} ~ s.t. ~(\phi_{pi} = j)\\
0~\textrm{otherwise}
\end{cases},
\end{equation}
and
\begin{equation}
\psi_{ij} = 
\begin{cases}
q~\textrm{if}~\exists! ~ q \in \mathbb{N} ~ s.t. ~(\phi_{iq} = j)\\
0~\textrm{otherwise}
\end{cases}
\end{equation}
We use $\xi_{ij}$ to retrieve the first antenna index of the composite antenna pair associated with a particular baseline, if the index of the first antenna in the composite antenna pair and the unique redundant baseline group index are known, while we use $\psi_{ij}$ to obtain 
a similar result: the second antenna index of the composite antenna pair. %\textbf{NB::} I still need to plot $\zeta_{ij}$ for different antenna layouts.

\subsection{Redundant Measurement Equation}
\label{sec:r_mes}
If the array is redundant and the function $\phi_{pq}$ is known we can rewrite Eq.~\eqref{eq:vis_definition} as
\begin{equation}
\label{eq:vis_red}
d_{pq} = g_{p}\conj{g_q}y_{\phi_{pq}} + n_{pq}.
\end{equation}
Eq.~\eqref{eq:vis_red} can also be expressed in the following vector form 
\begin{equation}
\label{eq:vis_red_vec}
\bd = \bv + \bn, 
\end{equation}
where 
\begin{align}
 \left [ \bd \right]_{\alpha_{pq}} &= d_{pq}, & \left [ \bv \right ]_{\alpha_{pq}} &= v_{pq}=g_p y_{\phi_{pq}} \conj{g_q},\nonumber\\
 \left [ \bn \right ]_{\alpha_{pq}} &= n_{pq}. &  &\label{eq:vec_definitions}
\end{align}
The vectors in Eq.~\eqref{eq:vec_definitions} are column vectors of size $B$ (i.e. $p<q$).

We have now defined enough notation to enable us to formulate redundant calibration as a least-squares problem:
\begin{equation}
\label{eq:least_squares_red}
\min_{\bz} \Lambda(\bz) = \min_{\bz} \|\br\|_F^2 = \min_{\bz} \|\bd - \bv(\bz)\|_F^2, 
\end{equation}
where
\begin{align}
 \bg &=[g_1,\cdots,g_N]^T, & \by &= [y_1,\cdots,y_L]^T,\nonumber\\
 \bz &= [\bg^T,\by^T]^T. &  &\label{eq:parm_definitions}
 \end{align}
The number of model parameters that we need to solve for in Eq.~\eqref{eq:least_squares_red} is equal to $P = 2(N+L)$, since redundant calibration is a complex problem.
We plot $L$ and $P$ for two different geometric layouts in Fig~\ref{fig:pl}.
The analytic expressions for the red and the blue curve in Fig.~\ref{fig:ln} are respectively equal to
\begin{equation}
L = 2N-\frac{1}{2}\sqrt{12N-3}-\frac{1}{2}, 
\end{equation}
and
\begin{equation}
L = N - 1. 
\end{equation}


\begin{figure*}
\centering
\subfigure[$L$]{\includegraphics[width=0.47\textwidth]{./ln.pdf}\label{fig:ln}}
% V_R_3.pdf: 585x441 pixel, 72dpi, 20.64x15.56 cm, bb=0 0 585 441
\subfigure[$P$]{\includegraphics[width=0.47\textwidth]{./pn.pdf}\label{fig:pn}}
\caption{In the left panel we plot the number of unique baseline groups $L$ one can form when using a hexagonal (red curve) and regular east-west layout (blue curve) as 
a function of the number of antennas $N$ in the array. In the right panel we plot the total number of parameters $P$ we need to estimate when employing redundant calibration for a hexagonal (red curve) and regular east-west layout (blue curve) as 
a function of the number of antennas $N$ in the array. 
\label{fig:pl}} 
\end{figure*}

\subsection{Signal-to-noise Ratio}
\label{sec:snr}
The following SNR definition is used in this paper  
\begin{equation}
\label{}
\textrm{SNR} = \frac{<\bv\odot\conj{\bv}>_{\nu,t,pq}}{<\bn\odot\conj{\bn}>_{\nu,t,pq}}, 
\end{equation}
where $<*>_{\nu,t,pq}$ denotes averaging over frequency, time and baselines and $\odot$ denotes the Hadamard product. The definition we use here is based on the SNR definitions used in \citet{Liu2010} and \citet{Marthi2014}.

\subsection{Paper Scope}
\label{sec:scope}
We only deal with per-timeslot and frequency channel calibration. No solution smoothing  
is applied between the different timeslots and channels. The reader interested in this is referred to \citep{Zheng2014}. 
Moreover, we do not explicitly deal with any of the degeneracies normally associated with redundant calibration \citep{Zheng2014,Kurien2016}. Similar to the approach taken in \citep{Marthi2014}, we assume that the degeneracies present in the redundant calibration solutions are removed with a final round of skymodel-based calibration.

\section{Redundant Calibration}
As discussed in Section~\ref{sec:ls}, we can solve Eq.~\eqref{eq:least_squares_red} in one of two ways. We can split the problem into its real and
imaginary parts and then solve for the real and imaginary parts of the parameters seperately or we can employ the notion of Wirtinger derivatives. The former 
approach is discussed in Section~\ref{sec:ri}, while the latter approach is described in detail in Section~\ref{sec:w}. 

\subsection{Real-Imag Framework}
\label{sec:ri}
As we mentioned in Section~\ref{sec:ls} using least-squares to solve complex problems can be problematic as $\bv$ needs to be analytic in its argument. In the case 
of Eq.~\eqref{eq:least_squares_red}, \citet{Liu2010} circumvents this difficulty by 
splitting the problem into its phase and amplitude components. 

The aforementioned is achieved by first rewriting Eq.~\eqref{eq:least_squares_red} to
\begin{equation}
\label{eq:least_squares_lincal}
\min_{\bmath{\varrho}} \|\breve{\br}\| = \min_{\bmath{\varrho}} \|\breve{\bd} - \breve{\bv}(\bmath{\varrho})\|, 
\end{equation}
where $\breve{\bd} = [\bd^T,\conj{\bd}^T]^T$, $\breve{\bv} = [\bv^T,\conj{\bv}^T]$ and $\bmath{\varrho}$ is the phase and amplitude parameter vector which we define in Appendix~\ref{sec:lincal}. 
With the aid of Eq.~\eqref{eq:least_squares_lincal} we can derive the \textsc{lincal} algorithm (see Appendix~\ref{sec:lincal}).
The Jacobian associated with Eq.~\eqref{eq:least_squares_lincal} is equal to Eq.~\eqref{eq:Jac_lin}. 
At the hand of Appendix~\ref{sec:lincal} it becomes evident that \textsc{lincal} is equivalent to a basic GN update step \citep{Kurien2016}. 
Therefore, a straightforward way of improving the convergence properties of stock standard \textsc{lincal} would be to simply add the LM damping factor to the basic \textsc{lincal} update step. 


\subsection{Complex Framework}
\label{sec:w}

\citet{Smirnov2015} recently proposed to use the Wirtinger derivative instead of the classic real and imaginary approach to perform normal skymodel-based calibration. We will now show
that the Wirtinger derivative can also be applied to redundant calibration.

%We can perform redundant calibration by solving the following optimization problem: 
%\begin{equation}
%\label{eq:least_squares_red}
%\min_{\bz} \Lambda(\bz) = \min_{\bz} \|\br\|_F^2 = \min_{\bz} \|\bd - \bv(\bz)\|_F^2, 
%\end{equation}
%where $\bz = [\bg,\by]^T$. The other vectors in Eq.~\eqref{eq:least_squares_red} were defined in Section~\ref{} \footnote{In this paper we use the same symbol to denote corrupted predicted visibilities and true observed corrupted visibilities as it
%is trivial to distinguish which one is implied from the context. Using the same symbol for these two quantities improves the readability of the paper.}. Note the similarity between Eq.~\eqref{eq:least_squares} and Eq.~\eqref{eq:least_squares_red},
%i.e. redundant calibration can be cast as a non-linear least squares problem. 

Analogous to \cite{Smirnov2015}, we now recast Eq.~\eqref{eq:least_squares_red} to 
\begin{equation}
\label{eq:least_squares_complex}
\min_{\breve{\bz}} \|\breve{\br}\| = \min_{\breve{\bz}} \|\breve{\bd} - \breve{\bv}(\breve{\bz})\|, 
\end{equation}
where $\breve{\bz} = [\bz^T,\conj{\bz}^T]^T$. Eq.~\eqref{eq:least_squares_complex} is the complex conjugate augmented counterpart of Eq.~\eqref{eq:least_squares_red}.
If we use the Wirtinger derivatives, Eq.~\eqref{eq:wir}, instead of the classic notion of a derivative to define the gradient operator then 
$\bv$ becomes analytic in $\bz$ and $\conj{\bz}$ as a whole \citep{Smirnov2015}. The fact that $\bv$ is now analytic in its argument has the immediate consequence: it becomes possible to apply complex GN
and complex LM to Eq.~\eqref{eq:least_squares_complex}. 

\subsubsection{Complex GN and LM}
In this section we discuss how we can use the GN and LM algorithms to solve Eq.~\eqref{eq:least_squares_complex} (i.e. estimate $\breve{\bz}$). The complex Jacobian is defined to be
\begin{equation}
\label{eq:Jacobian}
\bJ = \begin{bmatrix}
       \bmJ & \bmJ^*\\
       \conj{\bmJ}^* & \conj{\bmJ} 
      \end{bmatrix},
\end{equation}
where 
\begin{align}
\bmJ &= \frac{\partial \bv}{\partial \bz}, & \bmJ^* &= \frac{\partial \bv}{\partial \conj{\bz}}. 
\end{align}
In Eq.~\eqref{eq:Jacobian}, $\frac{\partial}{\partial \bz}$ and $\frac{\partial}{\partial \conj{\bz}}$ respectively denote the cogradient operator and the conjugate cogradient operator \citep{Sorber2012}.
The GN update step is now defined as:
\begin{equation}
\label{eq:GN_update}
 \Delta \breve{\bz} = (\bJ^H\bJ)^{-1}\bJ^H\breve{\br}.
\end{equation}
The LM update rule is very similar, the major difference being a damping factor $\lambda$ is introduced:
\begin{equation}
\label{eq:LM_update}
\Delta \breve{\bz} = (\bJ^H\bJ + \lambda\bD)^{-1}\bJ^H\breve{\br},
\end{equation}
where $\bD=\bI\odot\bJ^H\bJ$. We refer to $\bJ^H\bJ$ as the Hessian matrix $\bH$ and to $\bJ^H\bJ + \lambda\bD$ as the modified Hessian matrix $\bmH$. 

We use Eq.~\eqref{eq:GN_update} or Eq.~\eqref{eq:LM_update} to iteratively update our 
parameter vector as follows:
\begin{equation}
\label{eq:update}
\breve{\bz}_{k+1} = \breve{\bz}_{k} + \Delta \breve{\bz}_{k}. 
\end{equation}
We update our parameter vector by using Eq.~\eqref{eq:update} until convergence is reached. To improve readability we do not explicitly denote the iteration subscript $k$ in 
Eq.~\eqref{eq:GN_update} and Eq.~\eqref{eq:LM_update}.

\subsubsection{Analytic Expressions}
If we apply the definition in Eq.~\eqref{eq:Jacobian} to Eq.~\eqref{eq:least_squares_complex} we obtain the following analytic result:
\begin{equation}
\label{eq:Jacobian_red}
\bJ = \begin{bmatrix}
       \bM & \bN\\
       \conj{\bN} & \conj{\bM}
      \end{bmatrix},
\end{equation}
where
\begin{equation}
\bM =\begin{bmatrix}
      \bO & \bP
     \end{bmatrix},
\end{equation}
and 
\begin{equation}
\bN = \begin{bmatrix}
       \bQ & \bzero
      \end{bmatrix}.
\end{equation}
Moreover,
\begin{equation}
\left [ \bO  \right ]_{\alpha_{pq},j} = \begin{cases}
                                         y_{\phi_{pq}}\conj{g_q} & \textrm{if}~p=j\\
                                         0  & \textrm{otherwise} 
                                        \end{cases},
\end{equation}

\begin{equation}
\left [ \bP  \right ]_{\alpha_{pq},j} = \begin{cases}
                                         g_p\conj{g_q} & \textrm{if}~\phi_{pq}=j\\
                                         0  & \textrm{otherwise} 
                                        \end{cases}
\end{equation}
and
\begin{equation}
\left [ \bQ  \right ]_{\alpha_{pq},j} = \begin{cases}
                                         g_py_{\phi_{pq}} & \textrm{if}~q=j\\
                                         0  & \textrm{otherwise} 
                                        \end{cases}
\end{equation}

%\begin{align}
%\left [ \bM_1 \right ]_{\alpha_{pq},j} &= y_{\phi_{pq}}\conj{g_q}\delta_p^j, & \left [ \bM_2 \right ]_{\alpha_{pq},j} &= g_p\conj{g_q}\delta_{\phi_{pq}}^j 
%\end{align}
%\begin{equation}
%\left [ \bN_1 \right ]_{\alpha_{pq},j} = g_py_{\phi_{pq}}\delta^j_q. 
%\end{equation}

It is now trivial to compute the Hessian $\bH$ by using Eq.~\eqref{eq:Jacobian_red}. If we substitute Eq.~\eqref{eq:Jacobian_red} into $\bJ^H\bJ$
we obtain 
\begin{equation}
\label{eq:red_H}
\bH = \bJ^H\bJ = 
\begin{bmatrix}
\bA & \bB\\
\conj{\bB} & \conj{\bA}
\end{bmatrix},
\end{equation}
where

\begin{align}
\bA &= \begin{bmatrix} \bC & \bD\\ \bD^H & \bE \end{bmatrix}, & \bB &= \begin{bmatrix} \bF & \bG\\ \bG^T & \bzero \end{bmatrix},
\end{align}

\begin{equation}
[\bC]_{ij} = 
\begin{cases}
 \sum_{k \neq i} \left | g_k \right |^2 \left | y_{\zeta_{ik}} \right |^2 & \textrm{if} ~ i=j\\
 0 & \textrm{otherwise}
\end{cases},
\end{equation}
\begin{equation}
[\bD]_{ij} = 
\begin{cases}
 g_i \conj{y}_j  \left | g_{\psi_{ij}} \right |^2  & \textrm{if} ~ \psi_{ij}\neq0\\
 0 & \textrm{otherwise}
\end{cases},
\end{equation}

\begin{equation}
[\bE]_{ij} = 
\begin{cases}
 \sum_{pq \in \mathcal{PQ}_i} \left | g_p \right |^2 \left | g_q \right |^2  & \textrm{if} ~ i=j\\
 0 & \textrm{otherwise}
\end{cases},
\end{equation}
\begin{equation}
[\bF]_{ij} = 
\begin{cases}
 g_i g_j  \left | y_{\zeta_{ij}} \right |^2  & \textrm{if} ~ i \neq j\\
 0 & \textrm{otherwise}
\end{cases},
\end{equation}
and
\begin{equation}
[\boldsymbol{G}]_{ij} = 
\begin{cases}
 g_i y_j  \left | g_{\xi_{ij}} \right |^2  & \textrm{if} ~ \xi_{ij}\neq0\\
 0 & \textrm{otherwise}
\end{cases}.
\end{equation}
Moreover, $(*)^T$ denotes matrix transposition and
\begin{equation}
\mathcal{PQ}_i = \left\{pq\in\mathbb{N}^2|(\phi_{pq} = i) \right\}.
\end{equation}

Furthermore, substituting Eq.~\eqref{eq:Jacobian_red} into $\bJ^H\breve{\br}$ results in
\begin{equation}
\bJ^H\breve{\br} = \begin{bmatrix}
                   \ba \\
                   \bb \\
                   \conj{\ba}\\
                   \conj{\bb}
                   \end{bmatrix},
\end{equation}
where
\begin{align}
\label{eq:ab}
\left [ \ba \right ]_i &= \sum_{k\neq i} g_k \widetilde{y}_{ik}r_{ik},  & \left [ \bb \right ]_i &= \sum_{pq\in\mathcal{PQ}_i}\conj{g}_p g_q r_{pq},
\end{align}
and
\begin{equation}
\widetilde{y}_{ik} = 
\begin{cases}
\conj{y}_{\zeta_{ik}} & \textrm{if}~k > i\\
y_{\zeta_{ik}} & \textrm{otherwise}
\end{cases}.
\end{equation}
Moreover $\ba$ and $\bb$ are both column vectors.

HAVE TO RECHECK THE ABOVE EQUATIONS DIRECTLY FROM THE CODE.
DEFINED ALL OPERATORS AND SYMBOLS?

% \begin{equation}
% \boldsymbol{J}^H\breve{\boldsymbol{d}} = 
% \begin{bmatrix}
% \sum_{k\neq i } g_k x_{ik}d_{ik}\\
% \sum_{pq \in \mathcal{I}_j} \conj{g}_p g_q d_{pq}\\
% \downarrow^{*}
% \end{bmatrix}
% \begin{matrix}% matrix for right braces 
% \coolrightbrace{\sum g_k x_{\zeta_{ik}}d_{ik}}{i = 1\cdots N}\\
% \coolrightbrace{\sum \conj{g}_p g_q d_{pq}}{j = 1\cdots L}\\
% \vphantom{\downarrow^{*}}
% \end{matrix}
% \end{equation}



\subsubsection{Simplifying GN}

The following identities,
\begin{align}
\label{eq:identities}
\frac{1}{3}\bJ\breve{\bz} &= \breve{\bv}, & \bJ^H\breve{\bv} &= \widetilde{\bH}\breve{\bz} 
\end{align}
are trivially established by mechanically showing that the left hand side of each identity in Eq.~\eqref{eq:identities} is equal to its right hand side.
In Eq.~\eqref{eq:identities}, $\widetilde{\bH}$ denotes $\bI\odot\bH$.

Replacing $\breve{\br}$ with $\breve{\bd}-\breve{\bv}$ in Eq.~\eqref{eq:GN_update} results in
\begin{equation}
\label{eq:temp_GN_eq}
\Delta \breve{\bz} = (\bJ^H\bJ)^{-1}\bJ^H(\breve{\bd}-\breve{\bv}), 
\end{equation}
Substituting the first identity of Eq.~\eqref{eq:identities} into Eq.~\eqref{eq:temp_GN_eq} and simplifying the result 
\begin{equation}
\label{eq:one_thrid}
\Delta \breve{\bz} = (\bJ^H\bJ)^{-1}\bJ^H\breve{\bd}-\frac{1}{3}\breve{\bz}.
\end{equation}
It now trivially follows that:
\begin{equation}
\label{eq:two_thrids}
\breve{\bz}_{k+1} = (\bJ^H\bJ)^{-1}\bJ^H\breve{\bd} + \frac{2}{3}\breve{\bz}_{k}. 
\end{equation}
Eq.~\eqref{eq:two_thrids} shows us that in the case of redundant calibration we can calculate the GN update step without calculating the residual. 
% The 
% normal skymodel-based equivalent of Eq.~\eqref{eq:two_thrids} is equal to \citep{Smirnov2015}:
% \begin{equation}
% \label{eq:one_half}
% \breve{\bg}_{k+1} = (\bJ^H\bJ)^{-1}\bJ^H\breve{\bd} + \frac{1}{2}\breve{\bg}_{k}, 
% \end{equation}
% where $\breve{\bg} = [\bg^T,\conj{\bg}^T]^T$
% 
% FIX TRANSPOSE PROBLEM

\section{Results}
\label{sec:results}
In the previous section we discussed the complex GN and LM algorithms and how they relate to redundant calibration. If we inspect Eq.~\eqref{eq:GN_update} and 
Eq.~\eqref{eq:GN_update} we see that the most expensive operation used by the GN and LM algorithms is the inversion of the Hessian $\bH$. The most logical 
way of speeding up the algorithm is to investigate whether we can do this inversion in some efficient way.

Two recent algorithms have been proposed that are able to invert $\bH$ in an efficient manner, namely the ADI \citep{Marthi2014} and PCG \citep{Liu2010} methods. 
Note that here we focus solely on comparing the computational complexity of inverting $\bH$ using either one of these two methods. Rating the LM or GN algorithms
using one of these approaches relative to each other once the computation complexity of inverting $\bH$ is known is trivial. The simulations we performed to compare these two 
methods are summarized in Section~\ref{sec:simulations}.

%We restrict ourselves to comparing the most 
%expensive operation of the two methods namely, computing the inverse of the Hessian $\bH$ (or modified Hessian $\bmH$ in the case of LM). It becomes trivial to rate these two 
%algorithms relative to each other once the computational complexity of the inverse operation is known. 

The basic difference between these two speedup algorithms can be understood by inspecting Fig.~\ref{fig:hessian}. The first observation we can make by inspecting 
Fig.~\ref{fig:hessian_reg} and Fig.~\ref{fig:hessian_hex} is that the diagonal entries of the Hessians are more significant than the off-diagonal entries.
It therefore makes sense to approximate the $\bH$ (or the modified Hessian $\bmH$) with its diagonal, which greatly simplifies the computational complexity of inverting 
$\bH$, i.e. it becomes $\mathbb{O}(P)$\footnote{The cost of constructing 
the diagonal is not included in this estimate.} \citep{Smirnov2015}. This diagonal-approximation approach is effectively ADI \citep{Marthi2014}. The exact details of this 
approach is discussed in Section~\ref{sec:adi}. We will also show how \citet{Marthi2014}, \citet{Smirnov2015} and \citet{Salvini2014} relate to each other in that same section. It should be noted that the ADI method actually refers to the complete LM update step using the approximate $\bH$ and not just 
to the inversion of the approximate $\bH$. Furhtermore, the name ADI also implies that with this algorithm we alternate between computing the 
gains, their conjugates and the true observed visibilities. %In Appendix~\ref{sec:sbc} we briefly summarize the ADI method that is used for skymodel-based calibration.

The second observation we can make from Fig.~\ref{fig:hessian_reg} and Fig.~\ref{fig:hessian_hex} is that the Hessians contain many zero entries, i.e. they are sparse matrices. This, in conjunction with the more significant diagonal entries observation, implies that the PCG method (presented in Section~\ref{sec:conj_grad}) 
can be applied to invert them in a computationally efficient manner. \citet{Liu2010} proposed the PCG method to compute the inverse of $\bH$, however the computational complexity of this approach was not analyzed therein.  
In Section~\ref{sec:pcg}, we do a proper computational complexity analysis of PCG. We find that the computational complexity of PCG is asymptotically bounded by $\mathbb{O}(P^2)$, which is 
a lot better than straightforward inversion which takes $\mathbb{O}(P^3)$. 

\begin{figure*}
\centering
\subfigure[Regular layout]{\includegraphics[width=0.47\textwidth]{./reg_hessian.pdf}\label{fig:hessian_reg}}
% V_R_3.pdf: 585x441 pixel, 72dpi, 20.64x15.56 cm, bb=0 0 585 441
\subfigure[Hexagonal layout]{\includegraphics[width=0.47\textwidth]{./hex_hessian.pdf}\label{fig:hessian_hex}}
\caption{The number of analytic terms out of which the entries of the Hessian $\bH$ consists for two different geometric layouts, namely a regular east-west grid with $N = 5$ (left panel) and 
a hexagonal grid with $N = 7$. The diagonal entries of these two Hessians are clearly more significant than their off-diagonal entries. Moreover, these two Hessians, also 
contain many zero-entries. Note that the location of the zero-entries are dependent on the geometry of the array layout.
\label{fig:hessian}} 
\end{figure*}

%Since we approximate the Hessian in the ADI method with its diagonal the computational complexity of its inverse becomes $\mathbb{O}(P)$\footnote{The cost of constructing 
%the diagonal is not included in this estimate.}. 

Even though the PCG method is computationally more expensive than the ADI method we can not arrive at a final conclusion at this point. With the ADI method 
we only approximate the inverse of the Hessian, while in the case of the PCG method we compute the exact inverse. So the question arises whether the PCG method 
will converge much faster than the ADI method even though each iteration (which includes an inversion) is more expensive. We investigate this question in Section~\ref{sec:comparison}.

\subsection{Simulations}
\label{sec:simulations}
%The main purpose of the simulations conducted in this paper is to genrate experimental results with which we can compare the computational complexity of ADI and PCG. We used an hexagonal geometric layout with an adjustable number of rings during the 
%simulations. 
For the skymodel we generated 100 sources, each source had a random flux value and position. The flux of the sources followed a Pareto distribution (with a shape parameter of 2), while their positions were uniformly 
distributed in a 3$^{\circ}$ by 3$^{\circ}$ square grid around the field center. The remaining fixed simulation parameters can all be found in Table~\ref{tab:fixed_parm}. 
We also employed three different gain error models which we summarize in Table~\ref{tab:gain_parm}. We found that it did not matter which gain error model we used during
our simulations, as the model we used did not have an impact on the results. Moreover, we had two major experimental setups, we either made use of simulated observations consisting of one frequency channel and multiple time-slots or one time-slot and multiple channels.
We describe these two experimental setups in more detail in Table~\ref{tab:ch_parm}. As was the case for the gain error models, the experimental setup 
that was used did not alter the simulation results. 

The main flow of a single simulation is as follow. We created corrupted visibilities, using either one of the experimental setups summarized in Table~\ref{tab:ch_parm} and
the fixed simulation parameters presented in Table~\ref{tab:fixed_parm}. We then used per time-slot and channel redundant calibration, i.e. we either used the ADI or PCG algorithm, to 
estimate the gains and the true observed visibilities. To obtain statistically significant results we performed, for both the ADI and PCG algorithms, multiple statistical independent simulations. 

%We compare ADI with PCG in Section \textbf{??}, by using the results generated from the simulations we performed using 
%these two methods.

%The results reported in this paper were generated using two major experimental setups, namely a single time-slot and multiple frequency channels setup or 
%a single frquency channel and multiple time-slots setup. The simulation parameters which where kept fixed in both these setups can be found in 
%Table~\ref{tab:fixed_parm}. The sky-model we used to simulate the visibilities were the same for both setups.   We used a hexagonal redundant layout for our antenna positions. We produced 
%results for different 

\begin{table*}
\centering
\caption{The following fixed simulation parameters were used to generate the results of this paper.}
\begin{tabular}{|c c c c c c c|} 
\hline
Declination & Latitude & Minimum baseline & Geometry& Sources & Flux & Spatial \\
\hline 
\hline
 $-74^{\circ}39'37.481''$ & $-30^{\circ}43'17.34''$ & 20 & Hexagonal &100 & Pareto (shape param. of 2) & $U[-3,3]$ (deg.) 
\end{tabular}
\label{tab:fixed_parm}
\end{table*}

\begin{table*}
\centering
\caption{The gain error models used in this paper. We have used the symbol $x$ here as a proxy as it can either refer to time-slots or channels. We either
performed our simulations over multiple time-slots and one frequency channel or one timeslot and multiple frequencey channels (see Table~\ref{tab:ch_parm}). We use $U$ to represent 
a uniform distribution. Moreover, $c$ in the left most column denotes the speed of light.}
\begin{tabular}{|c c c c|} 
\hline
Number tag & 1 & 2 & 3\\
Model & Sinusoidal: amplitude and phase & Sinusoidal: real and imaginary parts & Linear phase slope \\ [0.5ex] 
\hline\hline
Function & $(A+1)e^{jP}$ & $A\cos(2\pi fx+B)+1.5A+jC\sin(2\pi fx+D)$ & $e^{jP}$ \\ 
\hline
Parameters & $A=a\cos(2\pi fx +b)$  & $f=5$ & $P=\tau x$ \\
 & $P =c \cos(2\pi fx +d)$ & $A,C\sim U[0.5,10]$ & $\tau = \frac{l}{c}$ \\
 & $f=5$ & $B,D\sim U[0,2\pi]$ &  $l\sim U[5,50]$ (m)\\
 & $a\sim U[0.8,0.9]$ &  & \\ 
 & $c\sim U[0.5,5]$ &  &  \\ 
 & $b,d\sim U[0,2\pi]$ &  &  \\ 
\hline
\end{tabular}
\label{tab:gain_parm}
\end{table*}

\begin{table}
\centering
\caption{We generated results using two major experimental setups. We either used one frequency channel and multiple time-slots, or one time-slot and multiple frequency 
channels. The most important parameters used in realizing these two major setups are presented here. Note that we refer to the gain error models in Table~\ref{tab:gain_parm} in the 
last row of this table.}
\begin{tabular}{|c c c|} 
\hline
 & Setup 1 & Setup 2\\
\hline
\hline
 Num. channels & 1024 & 1\\
$\nu$-range & 100-200 MHz & 1.4 GHz\\
Num. timeslots & 1 & 50\\
%Hour angle &-1^h & [-1^h,1^h]\\
Gain error model (Table~\ref{tab:gain_parm}) & 2 & 1\\
\hline
\end{tabular}
\label{tab:ch_parm}
\end{table}

\subsection{Alternating Direction Implicit Method}
\label{sec:adi}
%In Section~\ref{sec:adi} we show how the ADI method used for skymodel-based calibration relates to the redundant ADI method presented in \citet{Marthi2014}. 

We start this section by briefly reviewing the ADI method as it pertains to skymodel-based calibration in Section~\ref{sec:sbc} \citep{Salvini2014,Smirnov2015}.
In Section~\ref{sec:red_c} we re-derive the redundant calibration ADI method presented in \citet{Marthi2014}. In \citet{Marthi2014} the ADI method is derived by taking the derivative of the 
objective function $\Lambda$ relative to $\bg$ and $\by$ and setting the result to naught. When derived in this way, its relation to the recent works \citep{Salvini2014,Smirnov2015} are unclear.
We will use the diagonal-approximate approach proposed in \citet{Smirnov2015} to show that the ADI algorithm presented in \citet{Marthi2014} is in actual fact redundant 
\textsc{StEfCal} \citep{Salvini2014}. 

\subsubsection{Skymodel-based Calibration}
\label{sec:sbc}
\textsc{StEFCal} is an alternating direction implicit method, in which the measurement equation, Eq.~\eqref{eq:vis_definition}, is linearized by assuming that the gains are known, but
that their conjugates are unknown \citep{Mitchell:MWA-cal,Salvini2014}. Under this assumption, the conjugates of the gains can be obtained trivially. Once the conjugates of the gains have been computed, the gains can be solved for. One can then alternate between computing the gains and their conjugates until convergence
is reached. By correctly exploiting the symmetry inherent in the calibration problem this alternating approach can be replaced by a single iterative new parameter estimation step. \cite{Salvini2014} also show that
the convergence of the algorithm can be improved if for every even iteration the basic new parameter estimation step is modified as follows: the current gain estimate is replaced by the average of the current gain estimate and the gain estimate of the previous odd iteration.
\textsc{StEFCal} lowers the computational complexity of normal skymodel-based calibration from $\mathbb{O}(N^3)$ to $\mathbb{O}(N^2)$. 

\citet{Smirnov2015} recently showed that one can also derive \textsc{StEFCal} by viewing normal skymodel-based calibration as a complex optimization problem.
They realized while inspecting the skymodel-based $\bH$ that its diagonal entries are much more significant than its off-diagonal entries.
As we mention in Section~\ref{sec:results}, the exact same conclusion can be reached if we inspect the Hessian $\bH$ that is associated with redundant calibration (see Fig.~\ref{fig:hessian}). 
By approximating the skymodel-based $\bH$ by its diagonal and substituting this approximate Hessian $\widetilde{\bH}$ back into the fundamental skymodel-based LM update step we obtain \citep{Smirnov2015} 
\begin{align}
\breve{\bg}_{k+1} &\approx \frac{1}{1+\lambda}\widetilde{\bH}^{-1}\bJ^H\breve{\bd} + \frac{\lambda}{1+\lambda} \breve{\bg}_k,\label{eq:stef_lambda}\\
 &= \alpha \widetilde{\bH}^{-1}\bJ^H\breve{\bd} + (1-\alpha)\breve{\bg}_k, \label{eq:stef_alpha}  
\end{align}

If we choose $\lambda$ to be zero, which corresponds to an $\alpha$ equal to one, we obtain the new parameter estimation step of \textsc{StEfCal}; used during odd iterations. On the other hand if we choose $\lambda$ equal to one, which corresponds
to an $\alpha$ equal to a half, we obtain the new parameter estimation step of \textsc{StEfCal}; used during even iterations. 

Furthermore, the simplified skymodel-based GN equivalent of Eq.~\eqref{eq:two_thrids} is equal to
\citep{Smirnov2015}
\begin{equation}
\label{eq:one_half}
\breve{\bg}_{k+1} = (\bJ^H\bJ)^{-1}\bJ^H\breve{\bd} + \frac{1}{2}\breve{\bg}_{k}. 
\end{equation}
Note that in Eq.~\eqref{eq:one_half} the influence that the estimate of the parameters from the previous iteration has on the current iteration is scaled by a half, while
in Eq.~\eqref{eq:two_thrids} it is scaled by two thirds. Note the similarity between the last terms of Eq.~\eqref{eq:stef_alpha} and Eq.~\eqref{eq:one_half} when $\alpha$ is a half.

\subsubsection{Redundant Calibration}
\label{sec:red_c}
Since the diagonal entries of the $\bH$ associated with redundant calibration are more significant than its off-diagonal entries it makes 
sense to try and approximate it with its diagonal (see Fig.~\ref{fig:hessian} and Eq.~\eqref{eq:red_H}).
If we substitute $\bJ^H\bJ$ with $\widetilde{\bH}$ (by its diagonal approximation) and replace $\breve{\br}$ with $\breve{\bd} - \breve{\bv}$ in Eq.~\eqref{eq:GN_update} we obtain
\begin{equation}
\label{eq:appr_GN}
 \Delta \breve{\bz} \approx \widetilde{\bH}^{-1}\bJ^H(\breve{\bd}-\breve{\bv})
\end{equation}
Utilizing the second identity in Eq.~\eqref{eq:identities} allows us to simplify Eq.~\eqref{eq:appr_GN} to
\begin{equation}
  \Delta \breve{\bz} \approx \widetilde{\bH}^{-1}\bJ^H\breve{\bd}-\breve{\bz}.
\end{equation}
It now trivially follows that
\begin{equation}
 \breve{\bz}_{k+1} \approx \widetilde{\bH}^{-1}\bJ^H\breve{\bd}.
\end{equation}
We obtain a similar result if we repeat the above procedure, only in this case we use Eq.~\eqref{eq:LM_update} instead,
namely
\begin{align}
\breve{\bz}_{k+1} &\approx \frac{1}{1+\lambda}\widetilde{\bH}^{-1}\bJ^H\breve{\bd} + \frac{\lambda}{1+\lambda} \breve{\bz}_k,\label{eq:lambda}\\
 &= \alpha \widetilde{\bH}^{-1}\bJ^H\breve{\bd} + (1-\alpha)\breve{\bz}_k, \label{eq:alpha}  
\end{align}
where $\alpha = \frac{1}{1+\lambda}$. Moreover, $\lambda = \frac{1-\alpha}{\alpha}$.
The analytic expression of $\bJ^H\breve{\bd}$ will be very similar to the analytic 
expression of $\bJ^H\breve{\br}$, the only difference being that in Eq.~\eqref{eq:ab} the letter $r$ would be replaced by a $d$. If we substitute the analytic expression
of $\bJ^H\breve{\bd}$ and $\widetilde{\bH}^{-1}$ (which is easily computed as the matrix is a diagonal matrix) into Eq.~\eqref{eq:alpha} we obtain the following two update rules:
\begin{equation}
\label{eq:g_update}
g_{i}^{k+1} = \alpha \frac{\sum_{j\neq i} g_j^k \widetilde{y}_{ij}^{~\!\!k} d_{ij}}{\sum_{j\neq i} |g_j^k|^2|y_{\zeta_{ij}}^k|^2} + (1-\alpha) g_i^k, 
\end{equation}
and
\begin{equation}
\label{eq:y_update}
y_{i}^{k+1} = \alpha \frac{\sum_{pq \in \mathcal{PQ}_i} \conj{g}_p^k g_q^k d_{pq}}{\sum_{pq \in \mathcal{PQ}_i}|g_p^k|^2|g_q^k|^2} + (1-\alpha) y_i^k. 
\end{equation}
The computational complexity of inverting $\widetilde{\bH}$ is $\mathbb{O}(P)$.

Eq.~\eqref{eq:g_update} and Eq.~\eqref{eq:y_update} are the exact same iterative new parameter estimators derived by \citet{Marthi2014}. Also note that Eq.~\eqref{eq:g_update} is the fundamental
gain estimator associated with \textsc{StEfCal}. We have therefore successfully used the diagonal approximation approach proposed by \citet{Smirnov2015} to re-derive the ADI method presented in 
\citet{Marthi2014} and in doing so we have shown that the algorithm in \citet{Marthi2014} is actually redundant StEfCal. We could also have shown this by using the alternating approach 
presented in \citet{Salvini2014}; which explains why, in this paper, we have been referring to the algorithm presented in \citet{Marthi2014} as the ADI method.

The question now arises what value should we use for $\alpha$? In \citet{Marthi2014}, they propose 
$\alpha = 0.3$. Although an exact mathematical proof for the optimal choice of $\alpha$ still eludes us, it is still possible to
venture a clever guess by studying what the choice of $\alpha$ is in the case of skymodel-based \textsc{StEFCal}. The value of $\alpha$ is equal to a half 
in the case of \textsc{StEFCal} (for even iterations); see Section~\ref{sec:sbc}. If we wish to mimic this behaviour for the redundant calibration use case we need to scale $\breve{\bz}$ by two thirds in Eq.~\eqref{eq:alpha} (see Eq.~\eqref{eq:two_thrids}) which implies that we should choose $\alpha$ equal to a third, which corresponds to a $\lambda$ equal to two.
We used $\lambda$ equal to two in this paper.

Optimality results of ADI were presented in \citet{Marthi2014}. We do not repeat their analysis here. We however, do present our own ADI results in Fig.~\ref{fig:prec_error} that confirm that the
ADI method can accurately estimate the antenna gains and the observed visibilities at different SNR values. To quantity the exact error in our estimates we used the precentage error $\beta$ defined 
as 
\begin{equation}
\label{eq:beta}
\beta = \frac{\sum_{pq} |v_{pq} - \widehat{v}_{pq} |^2}{\sum_{pq} |v_{pq}|^2}.
\end{equation}
We have already defined $v_{pq}$ in Eq.~\eqref{eq:vec_definitions}. Moreover, in Eq.~\eqref{eq:beta}, $\widehat{v}_{pq}$ denotes the ADI estimate of $v_{pq}$.  

\begin{figure}
\includegraphics[width=0.47\textwidth]{./prec_error.pdf} 
\caption{We plot the precentage error $\beta$ between the estimated 
predicted visibilities and the true predicted visibilities for different SNR values as a function of the number of antennas in the array $N$. The graph presented here was generated using the first experimental setup in Table~\ref{tab:ch_parm}.}
\label{fig:prec_error}
\end{figure}

% At this point there is no way of knowing if this is mere coincidence. DEF REWRITE THIS.
% 
% REWRTIE THIS IN A DIFFERENT WAY. START WITH CHENGULAR AND JUST MENTION THE LINK TO STEFCAL'S CHOICE OF ALPHA.
% 
% The update rules in Eq.~\eqref{eq:lambda} and Eq.~\eqref{eq:alpha} have been derived before, by Chengular... The approach they followed is discussed in the Appendix.
% 
% Interestingly enough, if we substitute $\alpha = 1$ into Eq.~\ref{eq:g_update} then it reduces to the skymodel-based StEFCal update rule. In Chengular they propose 
% an $\alpha = 0.3$.
% 
% In an attempt to merge the terminology that has independently develop in the general calibration and redundant calibration literature and to emphasize the close
% relationship between Stefcal and the approach derived by I will use the label Redunandat SteFCal (R-Stefcal) to refer to the NLS method proposed by ... 
% 
% ****\\
% 
% with $\alpha = \frac{1}{1+\lambda}$. Moreover $\lambda = \frac{1-\alpha}{\alpha}$. So in Marthi and Chengular (2014) they use an $\alpha$ of 0.3, which corresponds to an LM damping factor $\lambda$
% of $2\frac{1}{3}$.
% 
% NB:: NEED TO ADD SNR PLOTS HERE
% PLUS IMAGE OF HESSIAN FOR THE DISCUSSION

\subsection{Preconditioned Conjugate Gradient Method}
\label{sec:pcg}
In this section we investigate the computational complexity of the PCG method when applied to the redundant calibration problem. 
As we mentioned in Section~\ref{sec:pcg} there are two factors which influence the computational complexity of the PCG method, namely the spectral condition number and the sparsity of the matrix we wish to invert (see Eq.~\eqref{eq:cg_bound}). 
We used a $\lambda$ value of two, implying we used Eq.~\eqref{eq:LM_update} instead of Eq.~\eqref{eq:GN_update} throughout this section.
We respectively discuss the spectral condition number and the sparsity of the modified Hessian $\bmH$, which is a non-singular matrix, in Section~\ref{sec:scn} and Section~\ref{sec:sparsity}.
It is interesting to note, that the Hessian $\bH$ itself is singular which means that if we are to use Eq.~\eqref{eq:GN_update} instead we would need to be able to invert a singular matrix. In Section~\ref{sec:normal},
we showed that the PCG method would in fact be able to handle such a scenario.

\subsubsection{Spectral Condition Number}
\label{sec:scn}
In Section~\ref{sec:precon} we mentioned that the convergence properties of CG can be dramatically improved through preconditioning. Inspecting Fig.~\ref{fig:hessian} reveals that the Jacobian preconditioner
is a good candidate for the redundant use case. In Fig.~\ref{fig:kappa} we see that this choice of preconditioner performs very well, the value of the spectral condition number $\kappa$
becomes constant and independent of problem size, i.e. the number of antennas in the array. Preconditioning therefore effectively eliminates $\kappa$ from Eq.~\eqref{eq:cg_bound}. This result
is confirmed by Fig.~\ref{fig:cg_itr}. Fig.~\ref{fig:cg_itr} shows us that the number of major iterations needed by the preconditioned conjugate gradient method to invert $\bmH$ is independent of the number of antennas in the array.
Moreover, the number of major iterations required are also much smaller than the dimension of $\bmH$. 

%kappa + itr plots
\begin{figure*}
\centering
\subfigure[$\kappa$]{\includegraphics[width=0.47\textwidth]{./kappa.pdf}\label{fig:kappa}}
% V_R_3.pdf: 585x441 pixel, 72dpi, 20.64x15.56 cm, bb=0 0 585 441
\subfigure[Iterations required before and after Preconditioning]{\includegraphics[width=0.47\textwidth]{./cg_itr.pdf}\label{fig:cg_itr}}
\caption{The graphs presented here were generated using the second experimental setup in Table~\ref{tab:ch_parm}. In the left panel we have plotted the spectral condition number $\kappa$ of the modified Hessian $\bmH$, before (magenta and red curves) and after preconditioning (blue curve), at different SNR values as a function of the
number of antennas $N$ in the array. The right hand panel confirms this result. In the right hand panel we have plotted the number of major iterations (line 6 of Algorithm \ref{algo:CG}) required by the conjugate 
gradient method to invert $\bmH$, before (magenta and red curves) and after preconditioning (blue and green curves), at different SNR values as a function of the number of antennas $N$ in the array.   
\label{fig:kappa_itr}} 
\end{figure*}

\subsubsection{Sparsity}
\label{sec:sparsity}
% \begin{enumerate}
% \item $\boldsymbol{H}$: is a $(4N-2)\times(4N-2)$ or a $P \times P$ matrix. $N$ denotes the number of antennas and $P$ denotes the number of parameters. This matrix
% contains $6N^2 - 2N - 2$ non-zero entries. It contains $16N^2 - 16N + 4$ entries. We can construct this matrix with $4N^2 -  4N$ or $\frac{1}{4} P^2 -1$ elementary operations.
% \item $\boldsymbol{A}$: is a $(2N-1)\times(2N-1)$ matrix. This matrix has $N^2 + N - 1$ non-zero entries. We can construct this matrix with $2\frac{1}{2} (N^2 -  N)$ elementary operations.  
% \item $\boldsymbol{B}$: is a $(2N-1)\times(2N-1)$ matrix. This matrix has $2N^2 - 2N$ non-zero entries. We can construct this matrix with $1\frac{1}{2} (N^2 -  N)$ elementary operations.
% \item $\boldsymbol{C}$: is a $N\times N$ matrix. This matrix has $N$ non-zero entries. We can construct this matrix with $N^2-N$ elementary operations.
% \item $\boldsymbol{D}$: is a $N \times (N-1)$ matrix. This matrix has $\frac{1}{2} (N^2 -  N)$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations. 
% \item $\boldsymbol{E}$: is a $(N-1) \times (N-1)$ matrix. This matrix has $(N-1)$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations.  
% \item $\boldsymbol{F}$: is a $N \times N$ matrix. This matrix has $N^2 - N$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations.
% \item $\boldsymbol{G}$: is a $N \times (N-1)$ matrix. This matrix has $\frac{1}{2} (N^2 -  N)$ non-zero entries. We can construct this matrix with $\frac{1}{2} (N^2 -  N)$ elementary operations.
% \item $\boldsymbol{0}$: is a $(N-1) \times (N-1)$ all zero matrix. 
% \end{enumerate}

%We can construct $\boldsymbol{C}-\boldsymbol{G}$ in $O(N^2)$ 

The second factor that determines the computational complexity of the conjugate gradient method when used to perform redundant calibration is the amount of non-zero
entries in the modified Hessian matrix $\bmH$. 
%We have also shown in section that the first factor, the spectral condition number of $\bH$, can be reduced 
%to a constant by employing preconditioning and can therefore be completely eliminated from the computational complexity equation. 
A useful matrix quantity which we will use in this section is the sparsity factor $\gamma$ of a matrix, which is the ratio between the amount of zero entries and the total amount of entries in a matrix. We 
can therefore define $\gamma$ as follows:
\begin{equation}
 \gamma = \left (1 - \frac{m}{P^2} \right ) 
\end{equation}
It is trivial to determine an analytic expression for the sparsity ratio of $\bmH$ when our redundant array is in a regular east-west geometric configuration
The sparsity and the asymptotic sparsity ratio of $\bmH$ are equal to
\begin{align}
\gamma &= \frac{5N^2-7N+3}{8N^2-8N+2} & \gamma_{\infty} &= \lim_{N\rightarrow \infty}\gamma = \frac{5}{8} \label{eq:gamma}. 
\end{align}
However when our redundant array makes use of a more complicated geometric layout it becomes much more difficult to construct such expressions as $\bmH$ itself 
becomes much harder to express analytically. 

In Fig.~\ref{fig:gamma} we plot the empirically determined sparsity ratios for three different geometric layouts as a function of the number antennas in the array. Although, we can not explicitly calculate 
the limit of the hexagonal and square layout curves, Fig.~\ref{fig:gamma} seems to indicate that they do exist. Moreover, we also plot the left most equation
of Eq.~\eqref{eq:gamma} (red dashed line) and the right most equation of Eq.~\eqref{eq:gamma} (black dotted curve) in Fig.~\ref{fig:gamma}.  

We can now use the sparsity factor to compute the order of the computational complexity of inverting $\bmH$ (assuming the effect of $\kappa$ is negliable) with
\begin{equation}
P^{c} = (1 - \gamma)P^2.
\end{equation}
Solving for $c$ we obtain
\begin{align}
c &= \log_{P}(1 - \gamma) + 2 & c_{\infty} &= \lim_{N\rightarrow \infty} c = 2. \label{eq:c}
\end{align}
Note, that we can only compute $c_{\infty}$ in the rightmost equation if $\lim_{N\rightarrow \infty} \gamma$ exists. 
In Fig.~\ref{fig:c}, we plot the empiracally determined value of $c$ for three different geometric layouts as a function of the number antennas in the array.
Furhtermore, in Fig.~\ref{fig:c}, the analytic value of $c$ for the regular east-west grid is plotted as a dashed red curve, while the limit of this analytic curve is plotted with a dotted black curve. 

Eq.~\eqref{eq:gamma}, Eq.~\eqref{eq:c} and Fig.~\ref{fig:sparsity} indicate that the computational complexity, under the assumption that $\lim_{N\rightarrow \infty} \gamma$ exists, is asymptotically bounded by $O(P^2)$.
However, the computational complexity converges very slowly to its asymptotic value, and in general is equal to $O(P^{c})$, where $c \sim 1.7$ if a Hexagonal geometric layout containing less than 200 antennas is employed.

\begin{figure*}
\centering
\subfigure[$\gamma$]{\includegraphics[width=0.47\textwidth]{./sparsity.pdf}\label{fig:gamma}}
% V_R_3.pdf: 585x441 pixel, 72dpi, 20.64x15.56 cm, bb=0 0 585 441
\subfigure[$c$]{\includegraphics[width=0.46\textwidth]{./comp_order.pdf}\label{fig:c}}
\caption{In the left panel we plot the sparsity ratio $\gamma$ (the number of zero entries relative to the total number of entries in a matrix) of the modified Hessian $\bmH$ as a function of the number 
of antennas $N$ in the array for different geometries, namely an hexagonal grid (blue circles), a square grid (green crosses) and a regular east-west grid (red circles). For the regular east-west grid we are able to construct an analytical expression for $\gamma$ (the dashed red line). Taking the limit 
of this analytic expression produces the black dotted line. In the right panel we plot the order of the computational cost $c$ for inverting $\bmH$ as a function of the number 
of antennas in the array for different geometries. We use the same color scheme in the right and left panels. The red dashed line 
in the right panel is associated with the theoretical curve of $c$ we computed for the east-west regular grid. The limit of the red-dashed curve is two, which we 
have plotted with a black dotted line.
\label{fig:sparsity}} 
\end{figure*}


\subsection{Comparison}
\label{sec:comparison}
We are now in a position to make a final comparison between the ADI and PCG methods. Inverting $\bH$ by assuming it is a diagonal matrix (i.e. using the ADI method) is very cheap, while inverting $\bH$ using PCG is more expensive. On the other side of the coin,  one inversion with PCG is much more 
accurate than using the approximate inverse which is associated with the ADI method. The question is now, if the inverse using the PCG method is much more accurate will the LM implementation using it require much less LM-iterations than its ADI counterpart. Moreover, will the number 
of LM iterations PCG require be so few that it manages to beat the ADI method after all. To answer this we first need to compute the number of iterations, $k$, the LM implementation associated with the ADI method can make in the 
same time it takes the LM implementation associated with the PCG method to perform a single iteration. We can compute this with
\begin{align}
\label{eq:k}
 kP &\approx P^{1.7}, & k &\approx P^{0.7}.
\end{align}
Next we need to measure the amount of LM iterations we save by using the PCG method and compare it to Eq.\eqref{eq:k}. We make this comparison 
in Fig.~\ref{fig:out_diff}. Fig.~\ref{fig:out_diff} tells us that the ADI method will outperform the PCG method as the number of LM iterations we save by taking the 
full inverse is not enough to exceed $k$.

\begin{figure*}
\centering
\subfigure[Number of LM iterations required by ADI and PCG.]{\includegraphics[width=0.47\textwidth]{./outerloop.pdf}\label{fig:outerloop}}
% V_R_3.pdf: 585x441 pixel, 72dpi, 20.64x15.56 cm, bb=0 0 585 441
\subfigure[Difference between ADI and PCG.]{\includegraphics[width=0.47\textwidth]{./diff.pdf}\label{fig:diff}}
\caption{The graphs presented here were generated using the second experimental setup in Table~\ref{tab:ch_parm}. In the left panel we have the number of LM iterations 
that is required for the LM algorithm using the ADI (green and blue curve) and the PCG (magenta and red curve) methods to converge to a parameter error tolerance of 1e-6 whilst using different 
SNR values as a function of the number of antennas $N$ in the array. In the right panel we have the average amount of LM iterations (difference between the ADI and PCG curves in the left panel) we save (green and blue curve) by computing the full-inverse with the PCG method whilst using different SNR values.
The black curve is associated with $k$, the number of iterations the LM implementation using the ADI method can perform in the same time it takes the LM implementation using 
the PCG method to perform one iteration.
\label{fig:out_diff}} 
\end{figure*}

\section{Conclusion}
In Section~\ref{sec:} we showed that instead of using the traditional phase-and-amplitude approach to perform redundant calibration we can actually use a purely complex
framework. In this framework the unknown variables and their conjugates are treated as independent variables that have to be estimated. We have also 
proposed to use the LM algorithm, instead of the GN algorithm, to perform the actual parameter estimation within this complex framework. The LM algorithm exhibits better convergence properties than the 
GN algorithm. This has the immediate consequence that the amount of algorithmic layers that are needed to perform redundant calibration are reduced.
Them most expensive operation associated with the LM algorithm (as well as the GN algorithm) is the inversion of $\bH$.
We also compared the computational complexity of two previously proposed algorithms to efficiently invert $\bH$ for the redundant calibration use case, namely 
ADI and PCG. We showed that the computational complexity of the ADI method is $\mathbb{O}(P)$, while the computational complexity of the PCG method is asymptotically bounded
by $\mathbb{O}(P^2)$. Both of these algorithms perform much better than stock standard inversion, which has a computational complexity of $\mathbb{O}(P^3)$.
Although the ADI method does execute faster than the PCG method it is far less robust as it would be less straightforward to apply to other calibration uses cases. The reason being,
the diagonal approximation we make with might not be accurate



















% fiducial initial guess 
% 
% The following partial derivatives are now easily computed
% 
% \begin{eqnarray}
% \frac{c_{ij}}{\partial \eta_p} &=& y_{i-j} e^{\eta_p - i \varphi_p} e^{\eta_q - i \varphi_q}\\ 
% \frac{c_{ij}}{\partial \varphi_p} &=&  -i y_{i-j} e^{\eta_p - i \varphi_p} e^{\eta_q - i \varphi_q}\\
% \frac{c_{ij}}{\partial \eta_q} &=& y_{i-j} e^{\eta_p - i \varphi_p} e^{\eta_q - i \varphi_q}\\ 
% \frac{c_{ij}}{\partial \varphi_q} &=&  i y_{i-j} e^{\eta_p - i \varphi_p} e^{\eta_q - i \varphi_q}\\
% \frac{y_{i-j}}{\partial \varphi_q} &=&  e^{\eta_p - i \varphi_p} e^{\eta_q - i \varphi_q}
% \end{eqnarray}
% 
% 
% The Wirtinger derivative is used in the last equation.
% 
% \begin{eqnarray}
% c_{ij} &\approx& c_{ij}^0 + \Delta \eta_p(y_{i-j}^0 e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0}) + \Delta \eta_q(y_{i-j}^0 e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0})\\
% && -i\Delta\varphi_p (y_{i-j}^0e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0}) +i\Delta\varphi_q (y_{i-j}^0e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0})\\
% && y_{i-j}^1(y_{i-j}^0e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0})\\
% &\approx&  c_{ij}^0 + e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0}(y_{i-j}^1 + y_{i-j}^0( \Delta \eta_p+ \Delta \eta_q - i\Delta\varphi_p + i\Delta\varphi_p))
% \end{eqnarray}
% 
% \begin{equation}
% \label{eq:residual}
% r_{ij} = \delta_{ij} = c_{ij}-c_{ij}^0 = e^{\eta_p^0 - i \varphi_p^0} e^{\eta_q^0 - i \varphi_q^0}(y_{i-j}^1 + y_{i-j}^0( \Delta \eta_p+ \Delta \eta_q - i\Delta\varphi_p + i\Delta\varphi_p)) 
% \end{equation}
% 
% Eq.~\ref{eq:residual} allows us to construct the following linear system:
% 
% \begin{equation}
% \boldsymbol{J}[\boldsymbol{\Delta \eta},\boldsymbol{\Delta \varphi},\boldsymbol{\Delta y}]^T = \boldsymbol{r}, 
% \end{equation}
% 
% where $\boldsymbol{J}$ is equal to 
% \begin{equation}
% \boldsymbol{J} = \bigg [\overbrace{\frac{c_{pq}}{\partial \eta_p}}^{i= 1\cdots N},~\overbrace{\frac{c_{pq}}{\partial \varphi_p}}^{i= 1\cdots N},~\overbrace{\frac{c_{pq}}{\partial y_{t}}}^{t=1\cdots r_s} \bigg ] \bigg \} [pq] = 1\cdots N_{\textrm{b}} (p<q) 
% \end{equation}
% or
% \begin{equation}
% \boldsymbol{J} = \bigg [\frac{c_{pq}}{\partial \boldsymbol{\eta}},~\frac{c_{pq}}{\partial \boldsymbol{\varphi}},~\frac{c_{pq}}{\partial \boldsymbol{y}} \bigg ] \bigg \} [pq] = 1\cdots N_{\textrm{b}} (p<q). 
% \end{equation}
% 
% The last column is again a Wirtinger derivative.
% 
% Which means we can obtain the least-squares estimate as follows:
% 
% \begin{equation}
% [\boldsymbol{\Delta \eta},\boldsymbol{\Delta \varphi},\boldsymbol{\Delta y}]^T = [\boldsymbol{J}^H\boldsymbol{J}]^{-1}\boldsymbol{J}^H\boldsymbol{r}.
% \end{equation}




\bibliographystyle{mn2e}
\bibliography{paper}

\appendix

\section{\textsc{LOGCAL}}
\label{sec:logcal}
We can rewrite Eq.~\eqref{eq:vis_red} to \citep{Liu2010}
\begin{eqnarray}
\ln |d_{pq}| &=& \ln |g_p| + \ln |g_q| + \ln |y_{\phi_{pq}}| + s_{pq} \label{eq:logcal_amp}\\
\angle d_{pq} &=& \angle g_p - \angle g_q + \angle \phi_{pq} + t_{pq} \label{eq:logcal_phase}
\end{eqnarray}
where
\begin{align}
s_{pq} &= \mathscr{R} \left \{1 + \frac{n_{pq}}{g_p\conj{g_q}y_{\phi_{pq}}} \right \}, & t_{pq} &= \mathscr{I}\left \{1 + \frac{n_{pq}}{g_p\conj{g_q}y_{\phi_{pq}}} \right \}.
\end{align}
Moreover, $\mathscr{R}\{*\}$ and $\mathscr{I}\{*\}$ denote the real and imaginary operators. 

With the aid of Eq.~\eqref{eq:logcal_amp} we can construct the following linear system:
\begin{equation}
\label{eq:log_system}
\bdelta = \bJ\bzeta + \bs, 
\end{equation}
where
\begin{align}
\left [ \bdelta \right ]_{\alpha_{pq}} &= \ln |d_{pq}|, & \left [ \bs \right ]_{\alpha_{pq}} = s_{pq} 
\end{align}
and
\begin{equation}
\left [ \bzeta \right ]_{j} = \begin{cases} \ln |g_j| & \textrm{if}~j\leq N \\ \ln |y_{j-N}| & \textrm{otherwise} \end{cases}. 
\end{equation}
Furthermore,
\begin{equation}
\bJ = 
\begin{bmatrix}
\bN & \bM
\end{bmatrix}
\end{equation}
with 
\begin{equation}
[\bN]_{\alpha_{pq},j} = \begin{cases}
       1 & \textrm{if}~(p=j)~\textrm{or}~(q=j)\\
       0 & \textrm{otherwise}
      \end{cases}
\end{equation}
and
\begin{equation}
[\bM]_{\alpha_{pq},j} = \begin{cases}
       1 & \textrm{if}~\phi_{pq}=j\\
       0 & \textrm{otherwise}
      \end{cases}
\end{equation}

The least squares solution to Eq.\eqref{eq:log_system}, under the incorrect assumption that $s_{pq}$ is normally distributed, is given by
\begin{equation}
\bzeta = (\bA^T\bA)^{-1} \bA^T\bdelta. 
\end{equation}
How to correctly deal with the fact that $s_{pq}$ is not normally distributed is discussed in \citep{Liu2010}. A similar linear system can be constructed for Eq.~\eqref{eq:logcal_phase}. 
From the least squares solutions to the linear systems associated with Eq.~\eqref{eq:logcal_amp} and Eq.~\eqref{eq:logcal_phase} we can compute the antenna gains as well as the true visibilities.

\section{\textsc{lincal}}
\label{sec:lincal}
In this section we derive \textsc{lincal} \citep{Liu2010} from first principles.
%Assume that $\eta_p,\eta_q,\widetilde{\eta}_{\phi_{pq}},\varphi_p,\varphi_q$ and $\widetilde{\varphi}_{\phi_{pq}}$ are real valued variables.
We can rewrite Eq.~\eqref{eq:vis_red_vec} to
\begin{equation}
\label{eq:lincal_vec}
\bd = \bv(\bvarrho) + \bn, 
\end{equation}
where 
\begin{align}
\left [ \bv(\bvarrho) \right ]_{\alpha_{pq}} &= e^{\eta_p - i \varphi_p} e^{\widetilde{\eta}_{\phi_{pq}}- i \widetilde{\varphi}_{\phi_{pq}}} e^{\eta_q + i \varphi_q}\\
&= v_{pq},
\end{align}
and
\begin{equation}
\bvarrho = 
\begin{bmatrix}
\bmath{\eta} \\
\bvarphi \\
\widetilde{\bmath{\eta}} \\
\widetilde{\bvarphi}
\end{bmatrix},\qquad
\begin{aligned}
\bmath{\eta} &= [\eta_1, \eta_2,\cdots \eta_N]^T\\
 \bmath{\varphi} &= [\varphi_1, \varphi_2,\cdots \varphi_N]^T\\
 \widetilde{\bmath{\eta}} &= [\widetilde{\eta}_1, \widetilde{\eta}_2,\cdots \widetilde{\eta}_L]^T\\
 \widetilde{\bmath{\varphi}} &= [\widetilde{\varphi}_1, \widetilde{\varphi}_2,\cdots \widetilde{\varphi}_L]^T
\end{aligned}.
\label{eq:help}
\end{equation}
Note that the vectors $\bmath{\eta},~\bvarphi,~\widetilde{\bmath{\eta}}$ and $\widetilde{\bvarphi}$ are real valued.

The first order Taylor expansion of Eq.~\eqref{eq:lincal_vec} around the fiducial point $\bvarrho_0$ is equal to
\begin{equation}
\label{eq:d_lin}
\bd = \bv_0 + \bmJ_0 \Delta \bvarrho  + \bn.
\end{equation}
If we calculate the residual, dropping the reference to a specific fiducial point, we obtain: 
\begin{equation}
\label{eq:r_lin}
\br = \bd - \bv = \bmJ \Delta \bvarrho + \bn.
\end{equation}
Moreover,
\begin{equation}
\label{eq:jac_upper_lincal}
\bmJ = \begin{bmatrix}
       \bN & \bM 
       \end{bmatrix},
\end{equation}
where
\begin{align}
\bN &= \begin{bmatrix} \bO & \bP \end{bmatrix}, & \bM &= \begin{bmatrix} \bQ & \bR \end{bmatrix}, 
\end{align}
\begin{equation}
 [\bO]_{\alpha_{pq},j} =  \frac{\partial v_{pq}}{\partial \eta_j} = \begin{cases} 
    v_{pq} &\textrm{if}~(p=j)~\textrm{or}~(q=j)\\
    0 & \textrm{otherwise}
   \end{cases},
\end{equation}
\begin{equation}
 [\bP]_{\alpha_{pq},j} =  \frac{\partial v_{pq}}{\partial \varphi_j} = \begin{cases}
                                                                        i v_{pq} &\textrm{if}~ p=j\\
                                                                        -i v_{pq} &\textrm{if}~ q=j \\
                                                                        0 & \textrm{otherwise} 
                                                                       \end{cases}
\end{equation}
\begin{equation}
 [\bQ]_{\alpha_{pq},j} =  \frac{\partial v_{pq}}{\partial \widetilde{\eta}_j} = \begin{cases} 
    v_{pq} &\textrm{if}~\phi_{pq}=j\\
    0 & \textrm{otherwise}
   \end{cases},
\end{equation}
\begin{equation}
 [\bR]_{\alpha_{pq},j} =  \frac{\partial v_{pq}}{\partial \widetilde{\varphi}_j} = \begin{cases} 
    i v_{pq} &\textrm{if}~\phi_{pq}=j\\
    0 & \textrm{otherwise}
   \end{cases}.
\end{equation}
Augmenting the residual by taking into account its conjugate results in 
\begin{equation}
\breve{\br} = \bJ \Delta \bvarrho + \bn, 
\end{equation}
where $\breve{\br} = \begin{bmatrix}\br^T & \conj{\br}^T \end{bmatrix}^T$ and 
\begin{equation}
\label{eq:Jac_lin}
\bJ = 
\begin{bmatrix}
 \bmJ\\  
 \conj{\bmJ}
\end{bmatrix}.
\end{equation}
The solution to 
\begin{equation}
 \min_{\Delta \bvarrho} \| \breve{\br} - \bJ \Delta \bvarrho\|_F^2,
\end{equation}
is equal to 
\begin{equation}
\label{eq:LINCAL_update}
\Delta \bvarrho = (\bJ^H\bJ)^{-1} \bJ^H\breve{\br}. 
\end{equation}
Eq.~\eqref{eq:LINCAL_update} is used to iteratively determine a new estimate of the parameter vector $\bvarrho$ as follows
\begin{equation}
\label{eq:new_rho}
\bvarrho_{k+1} = \bvarrho_k + \Delta \bvarrho_k. 
\end{equation}
Note that we do not explicitly denote the $k$ index in Eq.~\eqref{eq:LINCAL_update}. We update our parameter vector until convergence is reached.   


\label{lastpage}
\end{document}