% mn2esample.tex
%
% v2.1 released 22nd May 2002 (G. Hutton)
%
% The mnsample.tex file has been amended to highlight
% the proper use of LaTeX2e code with the class file
% and using natbib cross-referencing. These changes
% do not reflect the original paper by A. V. Raveendran.
%
% Previous versions of this sample document were
% compatible with the LaTeX 2.09 style file mn.sty
% v1.2 released 5th September 1994 (M. Reed)
% v1.1 released 18th July 1994
% v1.0 released 28th January 1994

\documentclass[useAMS,usenatbib]{mn2e}

% If your system does not have the AMS fonts version 2.0 installed, then
% remove the useAMS option.
%
% useAMS allows you to obtain upright Greek characters.
% e.g. \umu, \upi etc.  See the section on "Upright Greek characters" in
% this guide for further information.
%
% If you are using AMS 2.0 fonts, bold math letters/symbols are available
% at a larger range of sizes for NFSS release 1 and 2 (using \boldmath or
% preferably \bmath).
%
% The usenatbib command allows the use of Patrick Daly's natbib.sty for
% cross-referencing.
%
% If you wish to typeset the paper in Times font (if you do not have the
% PostScript Type 1 Computer Modern fonts you will need to do this to get
% smoother fonts in a PDF file) then uncomment the next line
% \usepackage{Times}

%%%%% AUTHORS - PLACE YOUR OWN MACROS HERE %%%%%
\usepackage{subfigure}
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{graphicx}
\usepackage{mathtools}

\newcommand{\bz}{\bmath{z}}
\newcommand{\bA}{\bmath{A}}
\newcommand{\br}{\bmath{r}}
\newcommand{\bg}{\bmath{g}}
\newcommand{\bd}{\bmath{d}}
\newcommand{\bv}{\bmath{v}}
\newcommand{\bn}{\bmath{n}}
\newcommand{\by}{\bmath{y}}
\newcommand{\bJ}{\bmath{J}}
\newcommand{\bD}{\bmath{D}}
\newcommand{\bN}{\bmath{N}}
\newcommand{\bM}{\bmath{M}}
\newcommand{\conj}[1]{\overline{#1}}

\newcommand\coolover[2]{\mathrlap{\smash{\overbrace{\phantom{%
    \begin{matrix} #2 \end{matrix}}}^{\mbox{$#1$}}}}#2} 

\newcommand\coolunder[2]{\mathrlap{\smash{\underbrace{\phantom{%
    \begin{matrix} #2 \end{matrix}}}_{\mbox{$#1$}}}}#2}

\newcommand\coolleftbrace[2]{%
#1\left\{\vphantom{\begin{matrix} #2 \end{matrix}}\right.}

\newcommand\coolrightbrace[2]{%
\left.\vphantom{\begin{matrix} #1 \end{matrix}}\right\}#2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Unknown]{TBD}
\author[A. V. Raveendran and A. N. Other]{A. V. Raveendran$^{1}$\thanks{E-mail:
email@address (AVR); otheremail@otheraddress (ANO)} and A. N.
Other$^{2}$\footnotemark[1]\thanks{This file has been amended to
highlight the proper use of \LaTeXe\ code with the class file.
These changes are for illustrative purposes and do not reflect the
original paper by A. V. Raveendran.}\\
$^{1}$Indian Institute of Astrophysics, Bangalore 560034, India\\
$^{2}$Building, Institute, Street Address, City, Code, Country}
\begin{document}

\date{Accepted 1988 December 15. Received 1988 December 14; in original form 1988 October 11}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2002}

\maketitle

\label{firstpage}

\begin{abstract}
TBD
\end{abstract}

\begin{keywords}
circumstellar matter -- infrared: stars.
\end{keywords}

\section{Introduction}

Main contributions of the paper:

\begin{enumerate}
 \item Present a general framework which unifies all the techniques developed so far showing they are all related (LINCAL, non-linear estimator, etc ...). They are all non-linear
 least-squares techniques, i.e. they either employ Gauss-Newton or the Levenberg-Marquardt algorithms. We have to start with LINCAL showing that it is GN, then we have to motivate 
 why we want to use Oleg's complex optimization framework.
 \item Use Oleg's complex optimization framework to re-derive the non-linear technique proposed by Marthi and Chengular. The novelty lies, in the fact that by using Oleg's
 framework we can find analytic expressions for the Jacobian, the Hessian and the Jacobian-residual product (which is not even the case for LINCAL). State that the algorithm is 
 effectively related to SteFCal and is eff an independent rediscovery of SteFCal and an extension of it into the redundant domain.
 \item Also we present the array geometry function to help us make the derivations from Marthi and Chengular easier to read and understand for a general layout.
 \item We also mention at this point that in Oleg's paper the question is raised is there a fast way of computing the exact inverse, we then present this new method, which is called
 conjugate gradient method. We discuss the algorithm and the two things which bound its execution time. Which is $kappa$ and $m$ (spectral condition number and its sparsity). Then
 we explore both of these parameters.
 \end{enumerate}
 
 **************
 FLOW OF PAPER
 **************
 
 \begin{enumerate}
 \item We need the definition of visibilities as in Liu. DEFINE SNR HERE ALREADY together maybe with the sigma value of the noise.
 \item Introduce the array geometery function.
 \item Write down logcal and lincal in matrix form... ?
 \item Short discussion of least squares and jacobian and hessian's. General GN and LM update rules.
 \item Introduce redundant calibration as a least squares problem. We will use this general fact to derive both popular methods.
 \item Propose a possible solution witch leads to lincal. Mention that this approach works as in this form the function is differentiable (a taylor expansion in the 
 parameters exists). Maybe mention that we can also divide the problem into real and imaginary etc...
 \item Show how this relates to lincal for example ---> show lincal is GN.
 \item Now introduce complex optimization ---> Main motivation for switching to the alternative framework is that the the differentials are very simplistic. We wish to show that
 we can derive the method of Chengular.
 \item Do the derivation of Chengular. 
 \item Brief discussion abouth Chengular and SteFCal and Complex Optimization. Here I show that in Oleg's paper he re-derives an algorithm called SteFCal. Stefcal works
 on the basis of alternating direction implicit method. The first signs of achieving a similar algorithm already appeared in Stefan's conference paper in which the alternating
 idea was first proposed. Then I mention Chegular re-derives the expression by using partial derivatives and extends it to redundant. One could also have used the linear alternating
 approach. In an attempt to merge the terminology that has independently develop in the general calibration and redundant calibration literature and to emphasize the close
 relationship between Stefcal and the approach derived by I will use the label Redunandat SteFCal (R-Stefcal) to refer to the NLS method proposed by ... Lastly we mention that similarly
 to how Oleg re-derived stefcal in, we have achieved the same approach.
 \item Faster Exact inverse. A question that Oleg poses in his paper is, does there exist a faster way to take the exact inverse of JhJ? One that is almost linear, and
 implies that we can therefore implement the full LM algorithm. The aim here is of course to reduce the number of iterations that are required to converge by using the 
 full inverse instead which would hopefully provide enough of a speedup to compensate for the more expensive full-inverse. The algorithm we propose is the conjugate 
 gradient method.
 \item We give the images of the HESSIAN of both lincal and the complex method (number of terms). So we can mention that both are sparse and contain diagonal entries that 
 are more significant than the off-diagonal entries. What linear inversion approach can take advantage of both these phenomenon. One such technique is 
 cg. 
 \item Briefly discuss CG and how its computationally bounded by its condition number and its sparsity (how does the diagonal play a role).
 \item Simulation description
 \item Will CG improve things?
 \item Now first discuss the condition
 number of the Hessian before and after pre-conditioning (pre-codnitioning can only be applied if a good inverse of a matrix is known, if it is known then it can improve 
 the spectral condition number of a matrix. We present here the kappa and iteration number graphs for the HEXAGONAL layout. Although the
 \item Now we discuss the sparsity results. 
 \item Provide a table that theoretical compares R-StEFCal and SPARC.
 \item Number of outerloop iterations. 
 \item Timing results.
 \item Accuracy results.
 \item Maybe some freq simulations.
 \end{enumerate}
 
 We
 
 \section{Visibilities}
The observed visibility $d_{pq}$ measured by the baseline formed by antenna $p$ and $q$ can be described as

\begin{equation}
\label{eq:vis_definition}
d_{pq} = g_{p}\conj{g}_{q}y_{pq} + n_{pq},
\end{equation}
where $g_{p}$ and $g_{q}$ are the direction independent gains associated with antenna $p$ and $q$, $y_{pq}$ denotes the true visibility that baseline $pq$ measured
and $n_{pq}$ is the thermal noise component. Conjugation is denoted by $\conj{(*)}$. During the course of an actual observation the true value of $g_p$, $g_q$ and $y_{pq}$ are unknown and are the quantities which infact have 
to be estimated.

The real and imaginary components of the thermal noise is normally distributed with a mean of zero and a standard deviation that is equal to [NB - are the real and imaginary parts defined with same sigma?]  
\begin{equation}
\sigma = \frac{\sqrt{2}k_{B}T_{\textrm{sys}}}{A\eta\sqrt{\Delta \nu \tau}}, 
\end{equation}
where $k_B$ is Boltzmann's constant, $T_{\textrm{sys}}$ is equal to the system temperature, $A$ is the effective collecting area of each element in the array, $\eta$ is a dimensionless
efficiency factor, $\Delta \nu$ is the observational bandwidth and $\tau$ is the integration time per visibility. 

\subsection{Redundant Array Geometry Function}
If an array is redundant then its redundant baselines sample the exact same visibilites in the $uv$-plane, i.e. if baseline $pq$ and $rs$ are redundant then $y_{pq} = y_{rs}$. We can
therefore also use the following useful indexing strategy: we can use singular redundant baseline indexes instead of composite antenna pairs to label the true observed visibilities. 

Let $\phi_{ij}:\mathbb{N}^2\rightarrow\mathbb{N}^+$ be the function that maps composite antenna pairs to the unique redundant baseline indexes of an array. We refer to this function 
as the redundant array geometry function in this paper. This mapping is not symmetric as 
$\phi_{ij} = 0$ if $i>j$. We can also define the following symmetric variant of $\phi_{ij}$:
\begin{equation}
\zeta_{ij} = 
\begin{cases}
\phi_{ij}~\textrm{if}~i\leq j\\
\phi_{ji}~\textrm{if}~i>j
\end{cases}.
\end{equation} 
It is possible to construct a simple analytic expression for $\zeta_{ij}$ when our array is in an east-west regular configuration, namely $\zeta_{ij} = |j-i|$. 
It becomes increasingly more difficult to construct analytic expressions of $\zeta_{ij}$ for other more complicated array layouts. The empirically constructed symmetric geometry
functions of three different redundant layouts are depicted in Fig.~\ref{fig:geometry_function}. We denote the range of $\zeta_{ij}$ with $\mathcal{R}(\zeta_{ij})$. The maximal element 
that $\zeta_{ij}$ can ascertain is denoted by $L$ and can be construed as the maximal number of unique redundant baselines which can be formed for a given 
array layout. Alternatively, we can interpret the symmetric geometry functions 
in Fig.~\ref{fig:geometry_function} as matrices. In this alternative paradigm, $\zeta_{ij}$ denotes a matrix entry instead. The dimension of these so called geometry 
matrices are $N\times N$, where $N$ denotes the number of antennas in the array. 

\begin{figure*}
\centering
\subfigure[Hexagonal layout]
{\includegraphics[width=0.33\textwidth]{./HEX_lay.pdf}\label{fig:HEX_lay}}
\subfigure[Square layout]
{\includegraphics[width=0.33\textwidth]{./SQR_lay.pdf}\label{fig:SQR_lay}}
\subfigure[Regular east-west layout]
{\includegraphics[width=0.33\textwidth]{./REG_lay.pdf}\label{fig:REG_lay}}

\subfigure[Hexagonal: $\zeta_{pq}$]
{\includegraphics[width=0.33\textwidth]{./HEX_phi.pdf}\label{fig:HEX_phi}}
\subfigure[Square: $\zeta_{pq}$]
{\includegraphics[width=0.33\textwidth]{./SQR_phi.pdf}\label{fig:SQR_phi}}
\subfigure[Regular east-west: $\zeta_{pq}$]
{\includegraphics[width=0.33\textwidth]{./REG_phi.pdf}\label{fig:REG_phi}}
\caption{In the top panel we have three different redundant antenna layouts, namely a hexagonal (left), square (middle) and a regular east-west (right) layout. 
In the bottom panel we have the respective symmetric redundancy geometry functions for the layouts on the top panel. We used 91 antennas
to construct the the hexagonal layout, while a 100 antennas where used in the square and east-west layouts. The maximal amount of redundant baselines that can be formed for 
the hexagonal, square and east-west layouts in the top panel are 165, 180 and 99 baselines respectively.\label{fig:geometry_function}}
\end{figure*}

If the function $\phi_{ij}$ is known and one is given one of the two antenna indexes that together form a redundant baseline as well as the redundant baseline index itself then it is possible 
to calculate the unknown antenna index. This can be computed via the following two expressions:
\begin{equation}
\xi_{ij} = 
\begin{cases}
p~\textrm{if}~\exists! ~ p \in \mathbb{N} ~ s.t. ~(\phi_{pi} = j)\\
0~\textrm{otherwise}
\end{cases},
\end{equation}
and
\begin{equation}
\psi_{ij} = 
\begin{cases}
q~\textrm{if}~\exists! ~ q \in \mathbb{N} ~ s.t. ~(\phi_{iq} = j)\\
0~\textrm{otherwise}
\end{cases}
\end{equation}
We use $\xi_{ij}$ to retrieve the first antenna index of the composite antenna pair associated with a particular baseline, if the index of the first antenna in the composite antenna pair and the unique redundant baseline index are known, while we use $\psi_{ij}$ to obtain 
a similar result; the second antenna index of the composite antenna pair. \textbf{NB::} I still need to plot $\zeta_{ij}$ for different antenna layouts.

\subsection{Signal-to-noise Ratio}
If the array is redundant and the function $\phi_{ij}$ is known we can rewrite Eq.~\eqref{eq:vis_definition} as
\begin{equation}
\label{eq:vis_red}
d_{pq} = g_{p}\conj{g}_{q}y_{\phi_{pq}} + n_{pq}.
\end{equation}
Eq.~\eqref{eq:vis_red} can also be expressed in the following vector form 
\begin{equation}
\bd = \bv + \bn, 
\end{equation}
where 
\begin{eqnarray}
 \bd &=& [d_{pq}] \big \} ~ [pq] = 1\cdots B ~ (p<q),\nonumber\\
 \bv &=& [g_p y_{\phi_{pq}} \conj{g}_q]  \big \} ~ [pq] = 1\cdots B ~ (p<q),\nonumber\\
 \bn &=& [n_{pq}]  \big \} ~ [pq] = 1\cdots B ~ (p<q).\label{eq:vec_definitions}
\end{eqnarray}
In Eq.~\eqref{eq:vec_definitions}, $B$ denotes the number of baselines in the array.

We will be using the following SNR (signal-to-noise ratio) definition in this paper  
\begin{equation}
\textrm{SNR} = \frac{<\bv\odot\conj{\bv}>_{t,pq}}{<\bn\odot\conj{\bn}>_{t,pq}}, 
\end{equation}
where $<*>_{t,pq}$ denotes averaging over time and baselines and $\odot$ denotes the Hadamard product. The definition we use here is based on the SNR definitions used in \citet{Liu2010} and \citet{Marthi2014}.

\subsection{LOGCAL}
One of the best known algorithms that is currently used to perform redundant calibration involves the use of logarithms. If we ignore the noise term then we can 
rewrite Eq.~\eqref{eq:vis_red} as
\begin{eqnarray}
\ln |d_{ij}| &=& \ln |g_i| + \ln |g_j| + \ln |y_{\phi_{ij}}| \label{eq:logcal_amp}\\
\angle d_{ij} &=& \angle g_i - \angle g_j + \angle \phi_{ij} \label{eq:logcal_phase}
\end{eqnarray}

We can easily express Eq.~\eqref{eq:logcal_amp} with the following linear system
\begin{equation}
\widetilde{\bd} = \bA\bz, 
\end{equation}
which has the following least-squares solution
\begin{equation}
\bz = (\bA^T\bA)^{-1}\bA^T\widetilde{\bd},
\end{equation}
where
\begin{equation}
\widetilde{\bd} = [\ln |d_{pq}|] \big \} ~ [pq] = 1,\cdots,B ~ (p<q),\nonumber\\ 
\end{equation}
and
\begin{equation}
\bA = 
\begin{bmatrix}
\coolover{{\scriptstyle j=1,\ldots,N}}{\delta^j_p\vee\delta^j_q} & \coolover{{\scriptstyle k=1,\ldots,L}\!\!\!\!}{~\delta_{\phi_{pq}}^k}
\end{bmatrix}
\begin{matrix}% matrix for right braces 
\coolrightbrace{\delta_{\phi_{pq}}^k}{[pq] = 1,\ldots,B~(p<q)}
\end{matrix}.
\end{equation}

Moreover, $(*)^T$ denotes the transpose of its operand, $(*)^{-1}$ denotes matrix inversion, $\vee$ denotes the logical or operator and
\begin{equation}
\delta_y^x  = \begin{cases}
          1 & \textrm{if}~x=y\\
          0 & \textrm{otherwise}
         \end{cases}.
\end{equation}
A similar linear system can be constructed for Eq.~\eqref{eq:logcal_phase}.

\subsection{LINCAL}
Assume that $\eta_i,\eta_j,\widetilde{\eta}_{\phi_{ij}},\varphi_i,\varphi_j$ and $\widetilde{\varphi}_{\phi_{ij}}$ are real valued variables.
Substituting $g_i$ with $e^{\eta_i - i \varphi_i}$, $g_j$ with $e^{\eta_j - i \varphi_j}$ and $y_{\phi_{ij}}$ with $e^{\eta_{\widetilde{\phi}_{ij}}- i \widetilde{\varphi}_{\phi_{ij}}}$ in Eq.~\eqref{eq:vis_red} results in  
\begin{equation}
\label{eq:lincal}
d_{ij} =  e^{\eta_i - i \varphi_i} e^{\widetilde{\eta}_{\phi_{ij}}- i \widetilde{\varphi}_{\phi_{ij}}} e^{\eta_j + i \varphi_j} + n_{ij}.
\end{equation}

A possible approach one can use to solve the unknowns in Eq.~\eqref{eq:lincal} is to write down the Taylor expansion of the first term of the right hand side of Eq.~\eqref{eq:lincal} around some 
initial fiducial guess of $\eta_i,\eta_j,\widetilde{\eta}_{\phi_{ij}},\varphi_i,\varphi_j$ and $\widetilde{\varphi}_{\phi_{ij}}$, which we denote with 
$\eta_i^0,\eta_j^0,\widetilde{\eta}_{\phi_{ij}}^0,\varphi_i^0,\varphi_j^0$ and $\widetilde{\varphi}_{\phi_{ij}}^0$, and then to solve the perturbations from this 
initial guess that minimize the difference between the right and left hand side of Eq.~\eqref{eq:lincal}. Once the perturbation values 
have been computed it is trivial to obtain an estimate of $g_i$, $g_j$ and $y_{\phi_{ij}}$. 

If we Taylor expand the first term on the right hand side of Eq.~\eqref{eq:lincal} around a fiducial initial guess we obtain
\begin{equation}
\label{eq:taylor_expansion}
d_{ij} = v_{ij}^0[1+\Delta \eta_i + \Delta \eta_j + i \varphi_i - i \Delta \varphi_j + \Delta \widetilde{\eta}_{\phi_{ij}} + i\Delta \widetilde{\eta}_{\phi_{ij}}] + n_{ij}, 
\end{equation}
where
\begin{equation}
v_{ij}^0 = e^{\eta_i^0 - i \varphi_i^0} e^{\widetilde{\eta}_{\phi_{ij}}^0 - i \widetilde{\varphi}_{\phi_{ij}}^0} e^{\eta_j^0 + i \varphi_j^0}, 
\end{equation}
and $\eta_i^0,\eta_j^0,\widetilde{\eta}_{\phi_{ij}}^0,\varphi_i^0,\varphi_j^0$ and $\widetilde{\varphi}_{\phi_{ij}}^0$ respectively denote the initial guess for $\eta_i,\eta_j,\widetilde{\eta}_{\phi_{ij}},\varphi_i,\varphi_j$ and $\widetilde{\varphi}_{\phi_{ij}}$.
Eq.~\eqref{eq:taylor_expansion} can be used to construct the following linear system
\begin{equation}
\breve{\br} = \bJ\Delta\widetilde{\bz}, 
\end{equation}
where 
\begin{equation}
\breve{\br} = [d_{ij}-v_{ij}^0] \big \} ~ [ij] = 1\cdots 2B, 
\end{equation}
\begin{equation}
\label{eq:jac_lincal}
\bJ = \begin{bmatrix}
      \bN & \bM\\
      \conj{\bN} & \conj{\bM}
      \end{bmatrix}
\end{equation}
and
\begin{equation}
\Delta\widetilde{\bz} = 
\begin{bmatrix}
\Delta \eta_i \\
\Delta \varphi_j \\
\Delta \widetilde{\eta}_k \\
\Delta \widetilde{\varphi}_l \\
\end{bmatrix}
\begin{matrix}% matrix for right braces 
\coolrightbrace{\eta_i}{i = 1\cdots N}\\
\coolrightbrace{\eta_i}{j = 1\cdots N}\\
\coolrightbrace{\eta_i}{k = 1\cdots L}\\
\coolrightbrace{\eta_i}{l = 1 \cdots L}
\end{matrix}.
\end{equation}

In Eq.~\eqref{eq:jac_lincal} 

\begin{eqnarray}
\bN &=& 
\begin{bmatrix}
\coolover{\!\!\!\!\!\!\!{\scriptstyle j=1,\ldots,N}}{\frac{\partial v_{pq}}{\partial \eta_j}} & \coolover{\!\!\!\!{\scriptstyle k=1,\ldots,N}}{~\frac{\partial v_{pq}}{\partial \varphi_k}}
\end{bmatrix}
\begin{matrix}% matrix for right braces 
\coolrightbrace{\frac{\partial v_{ij}}{\partial \eta_j}}{\scriptstyle [pq] = 1,\ldots,B~(p<q)}
\end{matrix}\\
&=& \begin{bmatrix}
\coolunder{\!\!\!\!\!\!\!{\scriptstyle j=1,\ldots,N}}{n_{j,pq}} & \coolunder{\!\!\!\!{\scriptstyle k=1,\ldots,N}}{(-1)^{\delta_q^k}i n_{k,pq}}
\end{bmatrix}
\begin{matrix}% matrix for right braces 
\coolrightbrace{\frac{\partial v_{ij}}{\partial \eta_j}}{\scriptstyle [pq] = 1,\ldots,B~(p<q)}
\end{matrix}\\
\end{eqnarray}

\begin{eqnarray}
\bM &=& 
\begin{bmatrix}
\coolover{\!\!\!\!\!\!\!{\scriptstyle j=1,\ldots,L}}{\frac{\partial v_{pq}}{\partial \widetilde{\eta}_j}} & \coolover{\!\!\!\!{\scriptstyle k=1,\ldots,L}}{~\frac{\partial v_{pq}}{\partial \widetilde{\varphi}_k}}
\end{bmatrix}
\begin{matrix}% matrix for right braces 
\coolrightbrace{\frac{\partial v_{ij}}{\partial \eta_j}}{\scriptstyle [pq] = 1,\ldots,B~(p<q)}
\end{matrix}\\
&=& \begin{bmatrix}
\coolunder{\!\!\!\!\!\!\!{\scriptstyle j=1,\ldots,N}}{m_{j,pq}} & \coolunder{\!\!\!\!{\scriptstyle k=1,\ldots,N}}{i m_{k,pq}}
\end{bmatrix}
\begin{matrix}% matrix for right braces 
\coolrightbrace{\frac{\partial v_{ij}}{\partial \eta_j}}{\scriptstyle [pq] = 1,\ldots,B~(p<q)}
\end{matrix}\\
\end{eqnarray}

Moreover,
\begin{equation}
n_{j,pq} = (\delta_{p}^{j}\vee\delta_{q}^{j}) v_{pq} 
\end{equation}
and
\begin{equation}
m_{j,pq} = \delta_{\phi_{pq}}^{j} v_{pq}.
\end{equation}

\section{Least Squares}
Scalar redundant calibration can be achieved by solving the following optimization problem
\begin{equation}
\min_{\bg,\by} \|\bd - \bv(\bg,\by)\|. 
\end{equation}
The most standard way of solving a least squares method is to use a gradient type of minimization algorithm to iteratively perturb the parameters so that the ... minimize the 
difference between observed and predicted visibilities. The gradient based minimization algorithms generally requires the model $\bv$ to be differentiable
towards each unknown. When the problem is complex, which is the case for Eq..., it becomes difficult to directly apply gradient methods to such problems. The reason being,
many complex functions are not analytic nor differentiable (no Taylor expansion exists) if we use classical definitions. For instance if the 
classical definition of differentiaton is used then $\frac{\partial z}{\partial \conj{z}}$, where $z \in \mathbb{C}$, does not exist.

The standard way of avoiding such complications is to divide the problem into its real and imaginary parts and then to solve for the real and imaginary parts of the parameters separately.
Recently cite the complex paper has proposed to extend the problem into [RW HERE NEED TO STATE BETTER] its conjugate domain and then to solve for the parameters and there complex conjugates. This can be accomplished
if we introduce Wirtinger derivatives. (I have to look into how to write this better).

The solution to Eq. can now be obtained by using a gradient type of minimization algorithm, like GN and LM. 

The GN update rule is given by\\

GN UPDATE RULE







% fiducial initial guess 
% 
% The following partial derivatives are now easily computed
% 
% \begin{eqnarray}
% \frac{c_{ij}}{\partial \eta_i} &=& y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\ 
% \frac{c_{ij}}{\partial \varphi_i} &=&  -i y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\
% \frac{c_{ij}}{\partial \eta_j} &=& y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\ 
% \frac{c_{ij}}{\partial \varphi_j} &=&  i y_{i-j} e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}\\
% \frac{y_{i-j}}{\partial \varphi_j} &=&  e^{\eta_i - i \varphi_i} e^{\eta_j - i \varphi_j}
% \end{eqnarray}
% 
% 
% The Wirtinger derivative is used in the last equation.
% 
% \begin{eqnarray}
% c_{ij} &\approx& c_{ij}^0 + \Delta \eta_i(y_{i-j}^0 e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}) + \Delta \eta_j(y_{i-j}^0 e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0})\\
% && -i\Delta\varphi_i (y_{i-j}^0e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}) +i\Delta\varphi_j (y_{i-j}^0e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0})\\
% && y_{i-j}^1(y_{i-j}^0e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0})\\
% &\approx&  c_{ij}^0 + e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}(y_{i-j}^1 + y_{i-j}^0( \Delta \eta_i+ \Delta \eta_j - i\Delta\varphi_i + i\Delta\varphi_i))
% \end{eqnarray}
% 
% \begin{equation}
% \label{eq:residual}
% r_{ij} = \delta_{ij} = c_{ij}-c_{ij}^0 = e^{\eta_i^0 - i \varphi_i^0} e^{\eta_j^0 - i \varphi_j^0}(y_{i-j}^1 + y_{i-j}^0( \Delta \eta_i+ \Delta \eta_j - i\Delta\varphi_i + i\Delta\varphi_i)) 
% \end{equation}
% 
% Eq.~\ref{eq:residual} allows us to construct the following linear system:
% 
% \begin{equation}
% \boldsymbol{J}[\boldsymbol{\Delta \eta},\boldsymbol{\Delta \varphi},\boldsymbol{\Delta y}]^T = \boldsymbol{r}, 
% \end{equation}
% 
% where $\boldsymbol{J}$ is equal to 
% \begin{equation}
% \boldsymbol{J} = \bigg [\overbrace{\frac{c_{pq}}{\partial \eta_i}}^{i= 1\cdots N},~\overbrace{\frac{c_{pq}}{\partial \varphi_i}}^{i= 1\cdots N},~\overbrace{\frac{c_{pq}}{\partial y_{t}}}^{t=1\cdots r_s} \bigg ] \bigg \} [pq] = 1\cdots N_{\textrm{b}} (p<q) 
% \end{equation}
% or
% \begin{equation}
% \boldsymbol{J} = \bigg [\frac{c_{pq}}{\partial \boldsymbol{\eta}},~\frac{c_{pq}}{\partial \boldsymbol{\varphi}},~\frac{c_{pq}}{\partial \boldsymbol{y}} \bigg ] \bigg \} [pq] = 1\cdots N_{\textrm{b}} (p<q). 
% \end{equation}
% 
% The last column is again a Wirtinger derivative.
% 
% Which means we can obtain the least-squares estimate as follows:
% 
% \begin{equation}
% [\boldsymbol{\Delta \eta},\boldsymbol{\Delta \varphi},\boldsymbol{\Delta y}]^T = [\boldsymbol{J}^H\boldsymbol{J}]^{-1}\boldsymbol{J}^H\boldsymbol{r}.
% \end{equation}




\bibliographystyle{mn2e}
\bibliography{paper}

\label{lastpage}

\end{document}
